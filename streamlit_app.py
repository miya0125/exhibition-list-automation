import streamlit as st
import pandas as pd
import os
import requests
from datetime import datetime
import re
import tempfile
import zipfile
import io
import logging
import csv
from charset_normalizer import detect
from notion_client import Client
import json

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å±•ç¤ºä¼šãƒªã‚¹ãƒˆè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé«˜æ©Ÿèƒ½ç‰ˆï¼‰",
    page_icon="ğŸ“Š",
    layout="wide"
)

# APIã‚­ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
DEFAULT_NOTION_API_KEY = "APIã‚’å…¥åŠ›"
DEFAULT_DATABASE_ID = "IDå…¥åŠ›"
DEFAULT_GOOGLE_SHEETS_API_KEY = "APIã‚’å…¥åŠ›"

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'merged_data' not in st.session_state:
    st.session_state.merged_data = pd.DataFrame()
if 'processed_files' not in st.session_state:
    st.session_state.processed_files = []
if 'processing_stats' not in st.session_state:
    st.session_state.processing_stats = {}

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è­¦å‘Šè¨­å®š
if 'memory_warning_shown' not in st.session_state:
    st.session_state.memory_warning_shown = False

# æ‹¡å¼µç‰ˆåˆ—åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¤§å¹…è¿½åŠ ï¼‰
COLUMN_RENAMES = {
    # ä¼šç¤¾åé–¢é€£
    'ä¼šç¤¾å': 'ä¼šç¤¾å', 'ä¼æ¥­å': 'ä¼šç¤¾å', 'å‡ºå±•ç¤¾å': 'ä¼šç¤¾å',
    'å‡ºå±•ç¤¾': 'ä¼šç¤¾å', 'æ³•äººå': 'ä¼šç¤¾å', 'COMPANY': 'ä¼šç¤¾å', 
    'ç¤¾å': 'ä¼šç¤¾å', 'Company': 'ä¼šç¤¾å', 'company': 'ä¼šç¤¾å',
    'å‡ºå±•ä¼æ¥­å': 'ä¼šç¤¾å', 'å‡ºå±•è€…å': 'ä¼šç¤¾å', 'Corporate Name': 'ä¼šç¤¾å',
    'çµ„ç¹”å': 'ä¼šç¤¾å', 'Organization': 'ä¼šç¤¾å', 'å›£ä½“å': 'ä¼šç¤¾å',
    'ä¼šç¤¾åã‚¿ã‚¤ãƒˆãƒ«': 'ä¼šç¤¾å',
    
    # æ‹…å½“è€…é–¢é€£
    'æ‹…å½“è€…å': 'æ‹…å½“è€…', 'æ°å': 'æ‹…å½“è€…', 'åå‰': 'æ‹…å½“è€…', 
    'æ‹…å½“': 'æ‹…å½“è€…', 'æ‹…å½“çª“å£': 'æ‹…å½“è€…', 'è²¬ä»»è€…': 'æ‹…å½“è€…', 
    'ã”æ‹…å½“è€…': 'æ‹…å½“è€…', 'Contact Person': 'æ‹…å½“è€…', 'Person': 'æ‹…å½“è€…',
    'å–¶æ¥­æ‹…å½“': 'æ‹…å½“è€…', 'ä»£è¡¨è€…': 'æ‹…å½“è€…', 'æ‹…å½“è€…æ§˜': 'æ‹…å½“è€…',
    'ãŠåå‰': 'æ‹…å½“è€…', 'Name': 'æ‹…å½“è€…', 'é€£çµ¡æ‹…å½“è€…': 'æ‹…å½“è€…',
    
    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹é–¢é€£
    'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'ãƒ¡ãƒ¼ãƒ«': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'e-mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'Eãƒ¡ãƒ¼ãƒ«': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'Email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 
    'E-Mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'Mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'E-mailã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'Emailã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'ãƒ¡ã‚¢ãƒ‰': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'Mail Address': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    
    # æ¥­ç•Œé–¢é€£
    'æ¥­ç•Œå': 'æ¥­ç•Œ', 'æ¥­ç•Œ': 'æ¥­ç•Œ', 'æ¥­ç¨®': 'æ¥­ç•Œ', 
    'åˆ†é‡': 'æ¥­ç•Œ', 'Industry': 'æ¥­ç•Œ', 'äº‹æ¥­åˆ†é‡': 'æ¥­ç•Œ',
    'ã‚«ãƒ†ã‚´ãƒªãƒ¼': 'æ¥­ç•Œ', 'Category': 'æ¥­ç•Œ', 'éƒ¨é–€': 'æ¥­ç•Œ',
    'æ¥­æ…‹': 'æ¥­ç•Œ', 'Sector': 'æ¥­ç•Œ', 'ç”£æ¥­': 'æ¥­ç•Œ',
    
    # å±•ç¤ºä¼šåé–¢é€£
    'å±•ç¤ºä¼šå': 'å±•ç¤ºä¼šå', 'å±•è¦§ä¼š': 'å±•ç¤ºä¼šå', 'å±•ç¤ºä¼š': 'å±•ç¤ºä¼šå', 
    'EXPO': 'å±•ç¤ºä¼šå', 'ã‚¤ãƒ™ãƒ³ãƒˆå': 'å±•ç¤ºä¼šå', 'Event': 'å±•ç¤ºä¼šå',
    'Exhibition': 'å±•ç¤ºä¼šå', 'Show': 'å±•ç¤ºä¼šå', 'Fair': 'å±•ç¤ºä¼šå',
    'è¦‹æœ¬å¸‚': 'å±•ç¤ºä¼šå', 'Trade Show': 'å±•ç¤ºä¼šå', 'ã‚¤ãƒ™ãƒ³ãƒˆ': 'å±•ç¤ºä¼šå',
    
    # å±•ç¤ºä¼šåˆæ—¥é–¢é€£
    'å±•ç¤ºä¼šåˆæ—¥': 'å±•ç¤ºä¼šåˆæ—¥', 'é–‹å§‹æ—¥': 'å±•ç¤ºä¼šåˆæ—¥', 'åˆæ—¥': 'å±•ç¤ºä¼šåˆæ—¥',
    'é–‹å‚¬æ—¥': 'å±•ç¤ºä¼šåˆæ—¥', 'Start Date': 'å±•ç¤ºä¼šåˆæ—¥', 'ä¼šæœŸåˆæ—¥': 'å±•ç¤ºä¼šåˆæ—¥',
    
    # é›»è©±ç•ªå·é–¢é€£ï¼ˆã‚ˆã‚Šå¹…åºƒã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ï¼‰
    'é›»è©±ç•ªå·': 'Tel', 'Tel': 'Tel', 'é›»è©±': 'Tel', 'TEL': 'Tel', 'Phone': 'Tel', 
    'TEL_FAX': 'Tel', 'TELã¨FAX': 'Tel', 'tel': 'Tel', 'phone': 'Tel',
    'Telephone': 'Tel', 'Phone Number': 'Tel', 'é€£çµ¡å…ˆé›»è©±': 'Tel',
    'TELç•ªå·': 'Tel', 'Telç•ªå·': 'Tel', 'â„¡': 'Tel', 'ï¼´ï¼¥ï¼¬': 'Tel',
    
    # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆé–¢é€£
    'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ': 'Website', 'WEBã‚µã‚¤ãƒˆ': 'Website', 'HP': 'Website', 
    'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸': 'Website', 'URL': 'Website', 'Web': 'Website', 'web': 'Website',
    'Website URL': 'Website', 'ã‚¦ã‚§ãƒ–': 'Website', 'WEB': 'Website', 'ã‚µã‚¤ãƒˆ': 'Website',
    'Homepage': 'Website', 'Webã‚µã‚¤ãƒˆURL': 'Website', 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURL': 'Website',
    
    # éƒµä¾¿ç•ªå·é–¢é€£
    'éƒµä¾¿ç•ªå·': 'YuubinBangou', 'ã€’': 'YuubinBangou', 'ZIPã‚³ãƒ¼ãƒ‰': 'YuubinBangou', 
    'ZIP': 'YuubinBangou', 'Zip Code': 'YuubinBangou', 'Postal Code': 'YuubinBangou',
    'éƒµä¾¿': 'YuubinBangou', 'ã‚†ã†ã³ã‚“ç•ªå·': 'YuubinBangou',
    
    # ä½æ‰€é–¢é€£
    'ä½æ‰€': 'Address', 'æ‰€åœ¨åœ°': 'Address', 'ã‚¢ãƒ‰ãƒ¬ã‚¹': 'Address', 'æœ¬ç¤¾': 'Address', 
    'æœ¬ç¤¾æ‰€åœ¨åœ°': 'Address', 'æœ¬ç¤¾ä½æ‰€': 'Address', 'ä¼šç¤¾ä½æ‰€': 'Address',
    'Location': 'Address', 'äº‹æ¥­æ‰€': 'Address', 'å–¶æ¥­æ‰€': 'Address',
    'æ‰€åœ¨': 'Address', 'ç•ªåœ°': 'Address',
    
    # FAXé–¢é€£
    'FAXç•ªå·': 'FAX', 'FAX': 'FAX', 'Fax': 'FAX', 'ãƒ•ã‚¡ãƒƒã‚¯ã‚¹': 'FAX',
    'fax': 'FAX', 'ãƒ•ã‚¡ã‚¯ã‚¹': 'FAX', 'Facsimile': 'FAX',
    
    # å•ã„åˆã‚ã›å…ˆé–¢é€£
    'å•ã„åˆã‚ã›å…ˆ': 'å•ã„åˆã‚ã›å…ˆ', 'é€£çµ¡å…ˆ': 'é€£çµ¡å…ˆ', 'ãŠå•ã„åˆã‚ã›å…ˆ': 'ãŠå•ã„åˆã‚ã›å…ˆ', 
    'Contact': 'é€£çµ¡å…ˆ', 'ãŠå•åˆã›å…ˆ': 'ãŠå•ã„åˆã‚ã›å…ˆ', 'å•åˆã›å…ˆ': 'å•ã„åˆã‚ã›å…ˆ',
    'Contact Information': 'é€£çµ¡å…ˆ', 'é€£çµ¡å…ˆæƒ…å ±': 'é€£çµ¡å…ˆ',
    
    # ãã®ä»–
    'å‚™è€ƒ': 'å‚™è€ƒ', 'ãƒ¡ãƒ¢': 'å‚™è€ƒ', 'Note': 'å‚™è€ƒ', 'Memo': 'å‚™è€ƒ', 'ã‚³ãƒ¡ãƒ³ãƒˆ': 'å‚™è€ƒ',
    'éƒ¨ç½²': 'éƒ¨ç½²', 'Department': 'éƒ¨ç½²', 'æ‰€å±': 'éƒ¨ç½²', 'Division': 'éƒ¨ç½²',
    'å½¹è·': 'å½¹è·', 'Title': 'å½¹è·', 'Position': 'å½¹è·', 'è‚©æ›¸': 'å½¹è·'
}

REQUIRED_COLUMNS = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "å±•ç¤ºä¼šå", "æ‹…å½“è€…", "æ¥­ç•Œ", "Tel", "ä¼šç¤¾å"]
KEY_COLS = ["å±•ç¤ºä¼šå", "æ¥­ç•Œ", "ä¼šç¤¾å"]

# ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»æ­£è¦åŒ–é–¢æ•°
def extract_email_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º"""
    if pd.isna(text) or text == "":
        return ""
    
    text = str(text)
    patterns = [
        r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
        r'([a-zA-Z0-9._%+-]+[@ï¼ ][a-zA-Z0-9.-]+[.ï¼][a-zA-Z]{2,})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            email = match.group(1)
            email = email.replace('ï¼ ', '@').replace('ï¼', '.')
            return email.lower()
    
    return ""

def extract_phone_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é›»è©±ç•ªå·ã‚’æŠ½å‡º"""
    if pd.isna(text) or text == "":
        return ""
    
    text = str(text)
    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼ˆï¼‰', '0123456789-()'))
    
    patterns = [
        r'(?:TEL|Tel|tel|é›»è©±|â„¡)[:ï¼š]?\s*([0-9\-\(\)\s]+)',
        r'(?:^|[\s])([0-9]{2,4}[\-][0-9]{2,4}[\-][0-9]{3,4})',
        r'(?:^|[\s])(\([0-9]{2,4}\)[0-9]{2,4}[\-]?[0-9]{3,4})',
        r'(?:^|[\s])([0-9]{10,11})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            phone = match.group(1).strip()
            if len(re.sub(r'[^0-9]', '', phone)) >= 10:
                return normalize_phone(phone)
    
    return ""

def normalize_phone(phone_str):
    """é›»è©±ç•ªå·ã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ•´ç†"""
    if pd.isna(phone_str) or phone_str == "":
        return phone_str
    
    phone_str = str(phone_str).strip()
    phone_str = phone_str.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼ˆï¼‰', '0123456789-()'))
    phone_str = re.sub(r'[^\d\-+\(\)]', '', phone_str)
    
    digits_only = re.sub(r'[^\d]', '', phone_str)
    if len(digits_only) == 10 and digits_only[0] == '0':
        return f"{digits_only[:2]}-{digits_only[2:6]}-{digits_only[6:]}"
    elif len(digits_only) == 11 and digits_only[0] == '0':
        return f"{digits_only[:3]}-{digits_only[3:7]}-{digits_only[7:]}"
    
    return phone_str

def validate_email(email):
    """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
    if pd.isna(email) or email == "":
        return email
    
    email = str(email).strip().lower()
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    
    if re.match(pattern, email):
        return email
    return ""

def is_google_sheet_url(url):
    """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã©ã†ã‹åˆ¤å®š"""
    patterns = [
        r"https://docs\.google\.com/spreadsheets/d/([a-zA-Z0-9-_]+)",
        r"https://drive\.google\.com/file/d/([a-zA-Z0-9-_]+)",
        r"https://drive\.google\.com/open\?id=([a-zA-Z0-9-_]+)"
    ]
    for pattern in patterns:
        match = re.match(pattern, url)
        if match:
            return match
    return None

def extract_sheet_id(url):
    """URLã‹ã‚‰ã‚·ãƒ¼ãƒˆIDã‚’æŠ½å‡º"""
    match = re.search(r"/spreadsheets/d/([a-zA-Z0-9-_]+)", url)
    if match:
        return match.group(1)
    
    match = re.search(r"/file/d/([a-zA-Z0-9-_]+)", url)
    if match:
        return match.group(1)
    
    match = re.search(r"[?&]id=([a-zA-Z0-9-_]+)", url)
    if match:
        return match.group(1)
    
    return None

def fix_email_address(email):
    """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ä¿®æ­£ã™ã‚‹é–¢æ•°"""
    if pd.isna(email) or str(email).strip() == '':
        return ''
    
    email = str(email).strip()
    
    # å…¨è§’ï¼ ã‚’åŠè§’@ã«å¤‰æ›
    email = email.replace('ï¼ ', '@')
    
    # ã‚«ãƒ³ãƒã‚’ãƒ”ãƒªã‚ªãƒ‰ã«å¤‰æ›ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã®ã¿ï¼‰
    if '@' in email:
        local_part, domain_part = email.split('@', 1)
        domain_part = domain_part.replace(',', '.')
        email = f"{local_part}@{domain_part}"
    
    # ã‚ˆãã‚ã‚‹èª¤å­—ã®ä¿®æ­£
    replacements = [
        ('@@', '@'),
        ('..', '.'),
        ('.comm', '.com'),
        ('co.jo', 'co.jp'),
        ('co..jp', 'co.jp'),
        ('co.jï½', 'co.jp'),
        ('cojp', 'co.jp'),
        ('.con', '.com'),
        ('.cm', '.com'),
        ('.cpm', '.com'),
        ('gmail.co', 'gmail.com'),
        ('yahoo.co,jp', 'yahoo.co.jp'),
    ]
    
    for wrong, correct in replacements:
        email = email.replace(wrong, correct)
    
    # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    email = email.replace('?', '')
    email = email.replace(' ', '')
    email = email.replace('ã€€', '')
    email = email.replace('\n', '')
    email = email.replace('\r', '')
    email = email.replace('\t', '')
    
    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã«å«ã¾ã‚Œã‚‹ã¹ãã§ãªã„æ–‡å­—ã‚’å‰Šé™¤
    email = re.sub(r'[^a-zA-Z0-9@._\-+]', '', email)
    
    # @ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€åˆã®@ä»¥å¤–ã‚’å‰Šé™¤
    at_count = email.count('@')
    if at_count > 1:
        parts = email.split('@')
        email = parts[0] + '@' + ''.join(parts[1:])
    
    # @ã®å‰å¾Œã«ä½•ã‚‚ãªã„å ´åˆã¯ç„¡åŠ¹
    if '@' not in email or email.startswith('@') or email.endswith('@'):
        return ''
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã®ä¿®æ­£
    if '@' in email:
        local, domain = email.split('@', 1)
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒæ•°å­—ã ã‘ã®å ´åˆã¯ç„¡åŠ¹
        if domain.isdigit():
            return ''
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãƒ”ãƒªã‚ªãƒ‰ãŒãªã„å ´åˆã€ä¸€èˆ¬çš„ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æ¨æ¸¬
        if '.' not in domain:
            if domain == 'gmail':
                domain = 'gmail.com'
            elif domain == 'yahoo':
                domain = 'yahoo.co.jp'
            elif domain in ['hotmail', 'outlook']:
                domain = domain + '.com'
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æœ€å¾ŒãŒãƒ”ãƒªã‚ªãƒ‰ã§çµ‚ã‚ã‚‹å ´åˆã¯å‰Šé™¤
        domain = domain.rstrip('.')
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å…ˆé ­ãŒãƒ”ãƒªã‚ªãƒ‰ã®å ´åˆã¯å‰Šé™¤
        domain = domain.lstrip('.')
        
        email = f"{local}@{domain}"
    
    # æœ€çµ‚çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    if not re.match(email_pattern, email):
        return ''
    
    return email.lower()

def merge_sns_columns(df):
    """SNSé–¢é€£ã®é‡è¤‡åˆ—ã‚’çµ±åˆã™ã‚‹é–¢æ•°"""
    try:
        # SNSã”ã¨ã®åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©
        sns_patterns = {
            'YouTube': ['YouTube', 'Youtube', 'YouTubeä¼æ¥­URL', 'Youtubeä¼æ¥­URL', 'youtube'],
            'Instagram': ['Instagram', 'instagram', 'Instagramä¼æ¥­URL', 'ã‚¤ãƒ³ã‚¹ã‚¿ã‚°ãƒ©ãƒ '],
            'Facebook': ['Facebook', 'facebook', 'Facebookä¼æ¥­URL', 'ãƒ•ã‚§ã‚¤ã‚¹ãƒ–ãƒƒã‚¯'],
            'Twitter': ['Twitter', 'twitter', 'Twitterä¼æ¥­URL', 'X', 'X_twitter', 'x'],
            'LinkedIn': ['LinkedIn', 'linkedin', 'linkedinä¼æ¥­URL', 'ãƒªãƒ³ã‚¯ãƒˆã‚¤ãƒ³'],
            'TikTok': ['TikTok', 'tiktok', 'TikTokä¼æ¥­URL', 'ãƒ†ã‚£ãƒƒã‚¯ãƒˆãƒƒã‚¯']
        }
        
        # å„SNSã«ã¤ã„ã¦åˆ—ã‚’çµ±åˆ
        for sns_name, patterns in sns_patterns.items():
            # è©²å½“ã™ã‚‹åˆ—ã‚’æ¢ã™
            matching_cols = []
            for col in df.columns:
                if any(pattern.lower() in str(col).lower() for pattern in patterns):
                    matching_cols.append(col)
            
            # è¤‡æ•°ã®åˆ—ãŒã‚ã‚‹å ´åˆã¯çµ±åˆ
            if len(matching_cols) > 1:
                # æœ€åˆã®éç©ºå€¤ã‚’å–å¾—
                df[sns_name] = df[matching_cols].apply(
                    lambda row: next((val for val in row if pd.notna(val) and str(val).strip()), ''), 
                    axis=1
                )
                # å…ƒã®åˆ—ã‚’å‰Šé™¤
                df = df.drop(columns=matching_cols)
                st.info(f"ğŸ”„ {sns_name}åˆ—ã‚’çµ±åˆã—ã¾ã—ãŸï¼ˆ{len(matching_cols)}åˆ— â†’ 1åˆ—ï¼‰")
            elif len(matching_cols) == 1:
                # 1åˆ—ã®ã¿ã®å ´åˆã¯åˆ—åã‚’çµ±ä¸€
                df = df.rename(columns={matching_cols[0]: sns_name})
        
        # é‡è¤‡ã™ã‚‹ä½æ‰€ã€é›»è©±ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—ã‚‚çµ±åˆ
        duplicate_patterns = {
            'Address': ['Address', 'Address_dup1', 'ä½æ‰€', 'ä½æ‰€.1', 'ä½æ‰€ã™ã¹ã¦'],
            'Tel': ['Tel', 'Tel_dup1', 'Tel_dup2', 'Tel_dup3', 'é›»è©±ç•ªå·', 'é›»è©±'],
            'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': ['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹_dup1', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼’'],
            'Website': ['Website', 'Website_dup1', 'WEB_Site', 'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URL', 'URL', 'URL2', 'URL_ã‚³ãƒ”ãƒ¼'],
            'ä¼šç¤¾å': ['ä¼šç¤¾å', 'ä¼šç¤¾å_dup1', 'ä¼šç¤¾å_dup2', 'ä¼šç¤¾å_dup3', 'ä¼šç¤¾å_dup4'],
            'æ¥­ç•Œ': ['æ¥­ç•Œ', 'æ¥­ç•Œ_dup1']
        }
        
        for target_name, patterns in duplicate_patterns.items():
            matching_cols = [col for col in df.columns if col in patterns]
            
            if len(matching_cols) > 1:
                # æœ€åˆã®éç©ºå€¤ã‚’å–å¾—
                df[target_name] = df[matching_cols].apply(
                    lambda row: next((val for val in row if pd.notna(val) and str(val).strip()), ''), 
                    axis=1
                )
                # å…ƒã®åˆ—ã‚’å‰Šé™¤ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆåã¯ä¿æŒï¼‰
                cols_to_drop = [col for col in matching_cols if col != target_name]
                df = df.drop(columns=cols_to_drop)
                st.info(f"ğŸ”„ {target_name}åˆ—ã‚’çµ±åˆã—ã¾ã—ãŸï¼ˆ{len(matching_cols)}åˆ— â†’ 1åˆ—ï¼‰")
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—ã®ä¿®æ­£ã‚’å®Ÿè¡Œ
        email_columns = []
        for col in df.columns:
            col_lower = str(col).lower()
            if any(keyword in col_lower for keyword in ['mail', 'email', 'ãƒ¡ãƒ¼ãƒ«', 'e-mail']):
                email_columns.append(col)
        
        if email_columns:
            fixed_count = 0
            for col in email_columns:
                original_emails = df[col].copy()
                df[col] = df[col].apply(fix_email_address)
                
                # ä¿®æ­£ã•ã‚ŒãŸä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                for orig, fixed in zip(original_emails, df[col]):
                    orig_str = str(orig).strip() if pd.notna(orig) else ''
                    fixed_str = str(fixed).strip() if pd.notna(fixed) else ''
                    if orig_str != fixed_str and orig_str != '':
                        fixed_count += 1
            
            if fixed_count > 0:
                st.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆ{fixed_count}ä»¶ï¼‰")
        
        return df
        
    except Exception as e:
        st.warning(f"SNSåˆ—çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return df

def google_sheet_to_csv_with_api(sheet_url, google_api_key=None):
    """Google Sheets APIã‚’ä½¿ç”¨ã—ã¦CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if not google_api_key:
        return None
    
    sheet_id = extract_sheet_id(sheet_url)
    if not sheet_id:
        return None
    
    # Google Sheets API URL
    api_url = f"https://sheets.googleapis.com/v4/spreadsheets/{sheet_id}/values/A:ZZ?key={google_api_key}"
    
    try:
        response = requests.get(api_url)
        response.raise_for_status()
        
        data = response.json()
        values = data.get('values', [])
        
        if not values:
            return None
        
        # CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
        import csv
        output = io.StringIO()
        writer = csv.writer(output)
        for row in values:
            writer.writerow(row)
        
        return output.getvalue().encode('utf-8')
        
    except Exception as e:
        st.warning(f"Google Sheets API ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def google_sheet_to_csv_url(sheet_url):
    """å…¬é–‹Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨ã®CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL"""
    sheet_id = extract_sheet_id(sheet_url)
    if not sheet_id:
        return None
    
    gid_match = re.search(r"[#&]gid=([0-9]+)", sheet_url)
    gid = gid_match.group(1) if gid_match else "0"
    return f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}"

def safe_concat_dataframes(chunk, debug_mode=False):
    """å®‰å…¨ã«DataFrameã‚’çµåˆã™ã‚‹é–¢æ•°"""
    if not chunk:
        return pd.DataFrame()
    
    if len(chunk) == 1:
        return chunk[0].copy().reset_index(drop=True)
    
    # å„DataFrameã‚’äº‹å‰å‡¦ç†
    safe_chunk = []
    for i, df in enumerate(chunk):
        try:
            # DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
            df_safe = df.copy()
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å®Œå…¨ã«ãƒªã‚»ãƒƒãƒˆ
            df_safe = df_safe.reset_index(drop=True)
            
            # åˆ—åã‚’æ–‡å­—åˆ—ã«çµ±ä¸€
            df_safe.columns = [str(col) for col in df_safe.columns]
            
            # åˆ—åã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£
            seen_cols = {}
            new_cols = []
            for col in df_safe.columns:
                if col in seen_cols:
                    seen_cols[col] += 1
                    new_col = f"{col}_dup{seen_cols[col]}"
                    new_cols.append(new_col)
                    if debug_mode:
                        st.warning(f"é‡è¤‡åˆ—åä¿®æ­£: {col} â†’ {new_col}")
                else:
                    seen_cols[col] = 0
                    new_cols.append(col)
            
            df_safe.columns = new_cols
            safe_chunk.append(df_safe)
            
        except Exception as e:
            if debug_mode:
                st.error(f"DataFrame{i}ã®å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    if not safe_chunk:
        return pd.DataFrame()
    
    # çµåˆã‚’è©¦è¡Œ
    try:
        # æ–¹æ³•1: é€šå¸¸ã®concat
        merged = pd.concat(safe_chunk, ignore_index=True, sort=False)
        return merged.reset_index(drop=True)
    except Exception as e1:
        if debug_mode:
            st.warning(f"é€šå¸¸ã®concatå¤±æ•—: {e1}")
        
        # æ–¹æ³•2: é€æ¬¡çš„ãªçµåˆ
        try:
            merged = safe_chunk[0].copy()
            for df in safe_chunk[1:]:
                # åˆ—ã‚’æƒãˆã‚‹
                for col in merged.columns:
                    if col not in df.columns:
                        df[col] = ""
                for col in df.columns:
                    if col not in merged.columns:
                        merged[col] = ""
                
                # åŒã˜åˆ—é †ã«ã™ã‚‹
                df = df[merged.columns]
                
                # çµåˆ
                merged = pd.concat([merged, df], ignore_index=True, sort=False)
                merged = merged.reset_index(drop=True)
            
            return merged
            
        except Exception as e2:
            if debug_mode:
                st.error(f"é€æ¬¡çµåˆã‚‚å¤±æ•—: {e2}")
            
            # æ–¹æ³•3: æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦æœ€åˆã®DataFrameã®ã¿è¿”ã™
            return safe_chunk[0].copy().reset_index(drop=True) if safe_chunk else pd.DataFrame()

def process_dataframe(df, filename):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®é«˜åº¦ãªå‡¦ç†ï¼ˆå®‰å…¨ç‰ˆï¼‰"""
    try:
        stats = {"email_extracted": 0, "tel_extracted": 0}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å±•ç¤ºä¼šåã‚’æ¨æ¸¬
        inferred_event_name = os.path.splitext(filename)[0]
        
        # é‡è¦ï¼šæœ€åˆã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
        df = df.reset_index(drop=True)
        
        # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
        columns_to_drop = []
        for col in df.columns:
            col_lower = str(col).lower().strip()
            # ãƒ–ãƒ¼ã‚¹ç•ªå·ã€å°é–“ç•ªå·ã€ãƒ­ã‚´URLã‚’å‰Šé™¤
            if any(keyword in col_lower for keyword in ['ãƒ–ãƒ¼ã‚¹ç•ªå·', 'å°é–“ç•ªå·', 'ãƒ­ã‚´url', 'ãƒ­ã‚´ url', 'logo', 'booth', 'å°é–“', 'ãƒ–ãƒ¼ã‚¹']):
                columns_to_drop.append(col)
                st.info(f"ğŸ—‘ï¸ ä¸è¦åˆ—å‰Šé™¤: {col}")
        
        if columns_to_drop:
            df = df.drop(columns=columns_to_drop)
        
        # ãƒ‡ãƒãƒƒã‚°: å…ƒã®åˆ—åã‚’è¡¨ç¤º
        st.write(f"ğŸ” **{filename}ã®å…ƒã®åˆ—å:**")
        st.write(list(df.columns))
        
        # Step 1: é‡è¤‡ã—ãŸåˆ—åã‚’ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£
        st.write("ğŸ”§ **é‡è¤‡åˆ—åãƒã‚§ãƒƒã‚¯ä¸­...**")
        original_columns = list(df.columns)
        
        # é‡è¤‡ã—ãŸåˆ—åã‚’æ¤œå‡º
        duplicate_counts = {}
        new_columns = []
        
        for col in original_columns:
            col_str = str(col).strip()
            if col_str in duplicate_counts:
                duplicate_counts[col_str] += 1
                new_col_name = f"{col_str}_dup{duplicate_counts[col_str]}"
                new_columns.append(new_col_name)
                st.warning(f"âš ï¸ é‡è¤‡åˆ—åæ¤œå‡º: '{col_str}' â†’ '{new_col_name}'")
            else:
                duplicate_counts[col_str] = 0
                new_columns.append(col_str)
        
        # åˆ—åã‚’æ›´æ–°
        df.columns = new_columns
        
        # åˆ—åã‚’æ–‡å­—åˆ—ã«çµ±ä¸€
        df.columns = [str(col) for col in df.columns]
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
        df = df.reset_index(drop=True)
        
        # Step 2: ç©ºã®åˆ—ã‚„ç„¡åŠ¹ãªåˆ—ã‚’å‰Šé™¤
        st.write("ğŸ§¹ **ç©ºåˆ—ã®å‰Šé™¤ä¸­...**")
        original_shape = df.shape
        
        # å®Œå…¨ã«ç©ºã®åˆ—ã‚’å‰Šé™¤
        df = df.dropna(axis=1, how='all')
        
        # åˆ—åãŒç©ºã‚„ç„¡åŠ¹ãªåˆ—ã‚’å‰Šé™¤
        valid_columns = []
        for i, col in enumerate(df.columns):
            col_str = str(col).strip()
            if col_str and col_str.lower() not in ['unnamed', 'nan', 'null', '']:
                valid_columns.append(col)
            else:
                st.info(f"ğŸ—‘ï¸ ç„¡åŠ¹ãªåˆ—ã‚’å‰Šé™¤: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹{i} '{col}'")
        
        df = df[valid_columns]
        
        if df.shape != original_shape:
            st.info(f"ğŸ“Š å½¢çŠ¶å¤‰æ›´: {original_shape} â†’ {df.shape}")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚‚è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        if len(df) > 0:
            st.write(f"ğŸ“„ **æœ€åˆã®3è¡Œã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:**")
            st.dataframe(df.head(3), use_container_width=True)
        
        # Step 3: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆé‡è¤‡å¯¾ç­–ï¼‰
        df = df.reset_index(drop=True)
        
        # åˆ—åæ­£è¦åŒ–ï¼ˆå³å¯†ãªãƒãƒƒãƒ”ãƒ³ã‚°ã®ã¿ä½¿ç”¨ï¼‰
        rename_dict = {}
        for col in df.columns:
            col_clean = str(col).strip()
            col_lower = col_clean.lower()
            
            # 1. å®Œå…¨ä¸€è‡´ã«ã‚ˆã‚‹å¤‰æ›ï¼ˆæœ€å„ªå…ˆï¼‰
            for old_name, new_name in COLUMN_RENAMES.items():
                if col_lower == old_name.lower():
                    rename_dict[col] = new_name
                    break
            
            # 2. å³å¯†ãªéƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå¿…è¦æœ€å°é™ã®ã¿ï¼‰
            if col not in rename_dict:
                # é›»è©±ç•ªå·ç³»ï¼ˆTELã€é›»è©±ã‚’å«ã‚€ï¼‰
                if any(keyword in col_lower for keyword in ['tel', 'ï¼´ï¼¥ï¼¬']) and 'url' not in col_lower:
                    if 'é›»è©±' in col_clean or 'tel' in col_lower.replace('ï¼´ï¼¥ï¼¬', 'tel'):
                        rename_dict[col] = 'Tel'
                
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ç³»
                elif any(keyword in col_lower for keyword in ['mail', 'ãƒ¡ãƒ¼ãƒ«']) and 'url' not in col_lower:
                    if 'address' in col_lower or 'ã‚¢ãƒ‰ãƒ¬ã‚¹' in col_clean:
                        rename_dict[col] = 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'
                
                # ä¼šç¤¾åç³»ï¼ˆURLã‚’é™¤å¤–ï¼‰
                elif 'url' not in col_lower and 'link' not in col_lower:
                    if any(keyword in col_clean for keyword in ['ä¼šç¤¾', 'ä¼æ¥­', 'ç¤¾å']):
                        rename_dict[col] = 'ä¼šç¤¾å'
                    elif any(keyword in col_lower for keyword in ['company']) and len(col_clean) < 20:
                        rename_dict[col] = 'ä¼šç¤¾å'
                
                # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆç³»ï¼ˆä¼æ¥­ã®å…¬å¼ã‚µã‚¤ãƒˆã®ã¿ï¼‰
                elif any(keyword in col_clean for keyword in ['ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ', 'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', 'HP']):
                    rename_dict[col] = 'Website'
                elif col_lower in ['url', 'website', 'homepage'] and len(col_clean) < 15:
                    rename_dict[col] = 'Website'
                
                # æ‹…å½“è€…ç³»
                elif 'url' not in col_lower and any(keyword in col_clean for keyword in ['æ‹…å½“', 'æ°å', 'åå‰']):
                    if 'ä¼šç¤¾' not in col_clean and 'ä¼æ¥­' not in col_clean:
                        rename_dict[col] = 'æ‹…å½“è€…'
                
                # ä½æ‰€ç³»
                elif 'url' not in col_lower and any(keyword in col_clean for keyword in ['ä½æ‰€', 'æ‰€åœ¨åœ°']):
                    rename_dict[col] = 'Address'
                
                # æ¥­ç•Œç³»
                elif 'url' not in col_lower and any(keyword in col_clean for keyword in ['æ¥­ç•Œ', 'æ¥­ç¨®', 'åˆ†é‡']):
                    rename_dict[col] = 'æ¥­ç•Œ'
                
                # å±•ç¤ºä¼šåˆæ—¥ç³»
                elif 'url' not in col_lower and any(keyword in col_clean for keyword in ['å±•ç¤ºä¼šåˆæ—¥', 'é–‹å§‹æ—¥', 'åˆæ—¥', 'é–‹å‚¬æ—¥']):
                    rename_dict[col] = 'å±•ç¤ºä¼šåˆæ—¥'
        
        # SNS URLs ã‚„ ãã®ä»–ã®URLã‚’æ˜ç¤ºçš„ã«é™¤å¤–
        excluded_patterns = [
            'facebook', 'twitter', 'instagram', 'linkedin', 'tiktok', 'youtube',
            'sns', 'social', 'æŠ½å‡ºå…ƒ', 'ãƒ­ã‚´', 'logo'
        ]
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã™ã‚‹åˆ—ã¯å¤‰æ›ã—ãªã„
        for col, new_name in list(rename_dict.items()):
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in excluded_patterns):
                if 'url' in col_lower or 'link' in col_lower:
                    del rename_dict[col]  # SNS URLã¯å¤‰æ›ã—ãªã„
        
        # ãƒ‡ãƒãƒƒã‚°: åˆ—åå¤‰æ›ã‚’è¡¨ç¤º
        if rename_dict:
            st.write(f"ğŸ”„ **åˆ—åå¤‰æ›:**")
            for old, new in rename_dict.items():
                st.write(f"- '{old}' â†’ '{new}'")
        else:
            st.write("â„¹ï¸ **åˆ—åå¤‰æ›ãªã—** (æ—¢ã«é©åˆ‡ãªåˆ—å)")
        
        # é™¤å¤–ã•ã‚ŒãŸåˆ—ã‚‚è¡¨ç¤º
        excluded_cols = [col for col in df.columns if col not in rename_dict and 
                        any(pattern in col.lower() for pattern in excluded_patterns + ['url', 'link'])]
        if excluded_cols:
            st.write(f"ğŸš« **é™¤å¤–ã•ã‚ŒãŸåˆ—ï¼ˆå¤‰æ›ã—ãªã„ï¼‰:**")
            for col in excluded_cols:
                st.write(f"- '{col}' (SNS/ãã®ä»–ã®URL)")
        
        # Step 4: åˆ—åå¤‰æ›å®Ÿè¡Œï¼ˆå†åº¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        st.write("ğŸ”„ **åˆ—åå¤‰æ›å®Ÿè¡Œä¸­...**")
        
        # å¤‰æ›å¾Œã®åˆ—åãŒé‡è¤‡ã—ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        new_column_names = []
        used_names = set()
        
        for col in df.columns:
            if col in rename_dict:
                new_name = rename_dict[col]
                if new_name in used_names:
                    # é‡è¤‡ã™ã‚‹å ´åˆã¯ç•ªå·ã‚’è¿½åŠ 
                    counter = 2
                    unique_name = f"{new_name}_{counter}"
                    while unique_name in used_names:
                        counter += 1
                        unique_name = f"{new_name}_{counter}"
                    new_column_names.append(unique_name)
                    used_names.add(unique_name)
                    st.warning(f"âš ï¸ å¤‰æ›å¾Œé‡è¤‡å›é¿: '{col}' â†’ '{unique_name}'")
                else:
                    new_column_names.append(new_name)
                    used_names.add(new_name)
            else:
                # å¤‰æ›ã—ãªã„åˆ—ã‚‚ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                original_name = col
                if original_name in used_names:
                    counter = 2
                    unique_name = f"{original_name}_{counter}"
                    while unique_name in used_names:
                        counter += 1
                        unique_name = f"{original_name}_{counter}"
                    new_column_names.append(unique_name)
                    used_names.add(unique_name)
                    st.warning(f"âš ï¸ å…ƒåˆ—åé‡è¤‡å›é¿: '{original_name}' â†’ '{unique_name}'")
                else:
                    new_column_names.append(original_name)
                    used_names.add(original_name)
        
        # åˆ—åã‚’ä¸€æ‹¬æ›´æ–°
        df.columns = new_column_names
        
        # åˆ—åã‚’æ–‡å­—åˆ—ã«çµ±ä¸€
        df.columns = [str(col) for col in df.columns]
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
        df = df.reset_index(drop=True)
        
        st.write(f"âœ… **å¤‰æ›å¾Œã®åˆ—å:**")
        st.write(list(df.columns))

        # é€£çµ¡å…ˆç³»åˆ—ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨TELæŠ½å‡º
        contact_cols = [c for c in df.columns if any(keyword in str(c).lower() for keyword in 
                       ['å•ã„åˆã‚ã›å…ˆ', 'é€£çµ¡å…ˆ', 'ãŠå•ã„åˆã‚ã›å…ˆ', 'contact', 'å•åˆã›å…ˆ', 'é€£çµ¡'])]
        
        # ãƒ‡ãƒãƒƒã‚°: é€£çµ¡å…ˆé–¢é€£ã®åˆ—ã‚’è¡¨ç¤º
        if contact_cols:
            st.write(f"ğŸ“ **é€£çµ¡å…ˆé–¢é€£ã®åˆ—:** {contact_cols}")
        
        if contact_cols:
            # å¿…è¦ã‚«ãƒ©ãƒ ã‚’äº‹å‰ã«ç¢ºä¿
            if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' not in df.columns:
                df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] = ''
            if 'Tel' not in df.columns:
                df['Tel'] = ''

            for col in contact_cols:
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
                temp_emails = df[col].apply(extract_email_from_text)
                email_mask = (df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] == '') & (temp_emails != '')
                if email_mask.any():
                    df.loc[email_mask, 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] = temp_emails[email_mask]
                    stats["email_extracted"] += email_mask.sum()
                    st.info(f"ğŸ“§ {col}ã‹ã‚‰{email_mask.sum()}ä»¶ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º")
                
                # é›»è©±ç•ªå·æŠ½å‡º
                temp_phones = df[col].apply(extract_phone_from_text)
                phone_mask = (df['Tel'] == '') & (temp_phones != '')
                if phone_mask.any():
                    df.loc[phone_mask, 'Tel'] = temp_phones[phone_mask]
                    stats["tel_extracted"] += phone_mask.sum()
                    st.info(f"ğŸ“ {col}ã‹ã‚‰{phone_mask.sum()}ä»¶ã®é›»è©±ç•ªå·ã‚’æŠ½å‡º")

        # å¿…é ˆåˆ—ãŒãªã‘ã‚Œã°è¿½åŠ 
        for col in REQUIRED_COLUMNS:
            if col not in df.columns:
                df[col] = ""
        
        # å±•ç¤ºä¼šåˆæ—¥åˆ—ã‚‚è¿½åŠ 
        if 'å±•ç¤ºä¼šåˆæ—¥' not in df.columns:
            df['å±•ç¤ºä¼šåˆæ—¥'] = ""
        
        # ãƒ‡ãƒãƒƒã‚°: å„å¿…é ˆåˆ—ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³ã‚’ç¢ºèª
        st.write(f"ğŸ“Š **å¿…é ˆåˆ—ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³:**")
        key_cols_check = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "Tel", "ä¼šç¤¾å"]
        for col in key_cols_check:
            if col in df.columns:
                non_empty_count = len(df[df[col].notna() & (df[col] != '')])
                total_count = len(df)
                st.write(f"- {col}: {non_empty_count}/{total_count}ä»¶ï¼ˆéç©ºï¼‰")
                
                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                sample_data = df[df[col].notna() & (df[col] != '')][col].head(3).tolist()
                if sample_data:
                    st.write(f"  ã‚µãƒ³ãƒ—ãƒ«: {sample_data}")
            else:
                st.write(f"- {col}: åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        
        # å±•ç¤ºä¼šåãŒç©ºã®å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
        if 'å±•ç¤ºä¼šå' in df.columns:
            df.loc[df['å±•ç¤ºä¼šå'] == '', 'å±•ç¤ºä¼šå'] = inferred_event_name
        
        # æ–‡å­—åˆ—åˆ—ã®å‰å¾Œç©ºç™½é™¤å»
        str_cols = df.select_dtypes(include="object").columns
        df[str_cols] = df[str_cols].apply(lambda s: s.str.strip() if hasattr(s, 'str') else s)
        
        # æ‹…å½“è€…åã®è£œå®Œï¼ˆä¿®æ­£ï¼šã€Œã”æ‹…å½“è€…ã€ã«å¤‰æ›´ï¼‰
        if "æ‹…å½“è€…" in df.columns:
            df["æ‹…å½“è€…"] = df["æ‹…å½“è€…"].fillna("").replace("", "ã”æ‹…å½“è€…")
        
        # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        if "Tel" in df.columns:
            df["Tel"] = df["Tel"].apply(normalize_phone)
        if "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹" in df.columns:
            df["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"] = df["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"].apply(validate_email)
        
        # å¿…é ˆ3åˆ—ã®æ¤œè¨¼ã‚’æ”¹å–„
        key_cols_check = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "Tel", "ä¼šç¤¾å"]
        validation_results = {}
        
        for col in key_cols_check:
            if col in df.columns:
                non_empty_data = df[df[col].notna() & (df[col] != '')]
                validation_results[col] = len(non_empty_data)
            else:
                validation_results[col] = 0
        
        # ã‚ˆã‚Šè©³ç´°ãªæ¤œè¨¼
        st.write(f"ğŸ” **ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ:**")
        for col, count in validation_results.items():
            st.write(f"- {col}: {count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š")
        
        # å°‘ãªãã¨ã‚‚1ã¤ã®åˆ—ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        total_useful_data = sum(validation_results.values())
        if total_useful_data == 0:
            raise ValueError(f"å¿…é ˆé …ç›®ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ»Telãƒ»ä¼šç¤¾åï¼‰ã«ãƒ‡ãƒ¼ã‚¿ãŒå…¨ãã‚ã‚Šã¾ã›ã‚“ã€‚åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æ¤œå‡ºã•ã‚ŒãŸåˆ—: {list(df.columns)}")
        
        # ä¼šç¤¾åãŒå…¨ãç©ºã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
        if validation_results.get("ä¼šç¤¾å", 0) == 0:
            raise ValueError("ä¼šç¤¾åã®ãƒ‡ãƒ¼ã‚¿ãŒå…¨ãã‚ã‚Šã¾ã›ã‚“ã€‚åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # ãƒ•ã‚¡ã‚¤ãƒ«åã¨æ›´æ–°æ—¥æ™‚ã‚’è¿½åŠ 
        df['ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«'] = filename
        df['æ›´æ–°æ—¥æ™‚'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        df['å‡¦ç†æœˆ'] = datetime.now().strftime('%Y-%m')
        
        # å±•ç¤ºä¼šåã‚’è¨­å®šï¼ˆå¿…é ˆåˆ—ã¨ã—ã¦è¿½åŠ ï¼‰
        if 'å±•ç¤ºä¼šå' not in df.columns:
            df['å±•ç¤ºä¼šå'] = ""
        
        # å±•ç¤ºä¼šåãŒç©ºã®å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
        empty_exhibition_mask = (df['å±•ç¤ºä¼šå'].isna()) | (df['å±•ç¤ºä¼šå'] == '')
        df.loc[empty_exhibition_mask, 'å±•ç¤ºä¼šå'] = inferred_event_name
        
        # æœ€çµ‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆï¼ˆé‡è¦ï¼‰
        df = df.reset_index(drop=True)
        
        st.success(f"âœ… {filename}: {len(df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«å‡¦ç†ã—ã¾ã—ãŸ")
        
        return df, stats, None
        
    except Exception as e:
        st.error(f"âŒ {filename} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        if 'df' in locals():
            st.write(f"ğŸ” **ãƒ‡ãƒãƒƒã‚°æƒ…å ±:**")
            st.write(f"- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å½¢çŠ¶: {df.shape}")
            st.write(f"- åˆ—å: {list(df.columns)}")
            st.write(f"- ãƒ‡ãƒ¼ã‚¿å‹: {df.dtypes.to_dict()}")
        
        return None, {}, str(e)

def notion_download():
    """Notion APIã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    st.subheader("ğŸ”— Notion APIã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    # APIè¨­å®š
    col1, col2 = st.columns(2)
    with col1:
        notion_api_key = st.text_input("Notion API Key", type="password", 
                                     value=os.environ.get("NOTION_API_KEY", DEFAULT_NOTION_API_KEY))
    with col2:
        database_id = st.text_input("Database ID", 
                                  value=os.environ.get("DATABASE_ID", DEFAULT_DATABASE_ID))
    
    # Google Sheets APIè¨­å®š
    st.markdown("### ğŸ”‘ Google Sheets APIè¨­å®šï¼ˆéå…¬é–‹ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨ï¼‰")
    
    with st.expander("ğŸ“‹ Google Sheets API Key ã®å–å¾—æ–¹æ³•"):
        st.markdown("""
        **1. Google Cloud Console ã«ã‚¢ã‚¯ã‚»ã‚¹:**
        - https://console.cloud.google.com/
        
        **2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã¾ãŸã¯é¸æŠ**
        
        **3. Google Sheets API ã‚’æœ‰åŠ¹åŒ–:**
        - ã€ŒAPIã¨ã‚µãƒ¼ãƒ“ã‚¹ã€â†’ã€Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€
        - ã€ŒGoogle Sheets APIã€ã‚’æ¤œç´¢ã—ã¦æœ‰åŠ¹åŒ–
        
        **4. èªè¨¼æƒ…å ±ã‚’ä½œæˆ:**
        - ã€ŒAPIã¨ã‚µãƒ¼ãƒ“ã‚¹ã€â†’ã€Œèªè¨¼æƒ…å ±ã€
        - ã€Œèªè¨¼æƒ…å ±ã‚’ä½œæˆã€â†’ã€ŒAPIã‚­ãƒ¼ã€
        
        **5. APIã‚­ãƒ¼ã‚’åˆ¶é™ï¼ˆæ¨å¥¨ï¼‰:**
        - ã€Œã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ¶é™ã€â†’ã€Œãªã—ã€
        - ã€ŒAPIã®åˆ¶é™ã€â†’ã€ŒGoogle Sheets APIã€ã®ã¿
        """)
    
    col1, col2 = st.columns(2)
    with col1:
        google_api_key = st.text_input(
            "Google Sheets API Key (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)", 
            type="password",
            value=os.environ.get("GOOGLE_API_KEY", DEFAULT_GOOGLE_SHEETS_API_KEY),
            help="éå…¬é–‹ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«å¿…è¦"
        )
    with col2:
        fallback_option = st.selectbox(
            "ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—æ™‚ã®å‡¦ç†",
            ["ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ", "ã‚¨ãƒ©ãƒ¼ã§åœæ­¢"],
            help="éå…¬é–‹ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„å ´åˆã®å‹•ä½œ"
        )
    
    # å‡¦ç†æ–¹æ³•ã®èª¬æ˜
    if google_api_key:
        st.info("ğŸ”‘ **å‡¦ç†æ–¹æ³•:** Google Sheets API â†’ å…¬é–‹URL â†’ ã‚¹ã‚­ãƒƒãƒ—/ã‚¨ãƒ©ãƒ¼")
    else:
        st.warning("âš ï¸ **å‡¦ç†æ–¹æ³•:** å…¬é–‹URLã®ã¿ï¼ˆéå…¬é–‹ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¯å¤±æ•—ã—ã¾ã™ï¼‰")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†é¡é¸æŠ
    st.markdown("### ğŸ¯ ãƒ‡ãƒ¼ã‚¿åˆ†é¡")
    filter_option = st.radio(
        "å–å¾—ãƒ‡ãƒ¼ã‚¿ã®åˆ†é¡æ–¹æ³•",
        ["ã™ã¹ã¦çµ±åˆ", "ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡"],
        help="ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡ï¼šãƒ¡ãƒ¼ãƒ«ã‚ã‚Šãƒ»TELã‚ã‚Šãƒ»URLã‚ã‚Šã§åˆ†ã‘ã¦å–å¾—"
    )
    
    if filter_option == "ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡":
        st.info("ğŸ“‚ ä»¥ä¸‹ã®3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†ã‘ã¦å–å¾—ãƒ»å‡¦ç†ã—ã¾ã™")
        
        # ã‚«ãƒ†ã‚´ãƒªé¸æŠã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.markdown("### ğŸ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            download_email = st.checkbox(
                "ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚ã‚Šãƒ•ã‚©ãƒ«ãƒ€",
                value=True,
                help="ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒã€Œã‚ã‚Šã€ã®ãƒ‡ãƒ¼ã‚¿"
            )
            if download_email:
                st.write("- ã‚ã‚Š")
        
        with col2:
            download_tel = st.checkbox(
                "ğŸ“ TELã‚ã‚Šãƒ•ã‚©ãƒ«ãƒ€",
                value=True,
                help="é›»è©±ç•ªå·æƒ…å ±ãŒã‚ã‚‹ãƒ‡ãƒ¼ã‚¿"
            )
            if download_tel:
                st.write("- TELã¨URL")
                st.write("- Telã€ä½æ‰€ã€URL")
        
        with col3:
            download_url = st.checkbox(
                "ğŸŒ URLã‚ã‚Šãƒ•ã‚©ãƒ«ãƒ€",
                value=True,
                help="URLæƒ…å ±ãŒã‚ã‚‹ãƒ‡ãƒ¼ã‚¿"
            )
            if download_url:
                st.write("- ç¤¾åãƒ»ä½æ‰€ãƒ»URL")
                st.write("- ç¤¾åã¨URLï¼ˆç›´ã§ä¼æ¥­HPãƒªãƒ³ã‚¯ï¼‰")
                st.write("- ç¤¾åã¨URLã®ã¿")
        
        # é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã‚’ä¿å­˜
        st.session_state.selected_categories = {
            "ğŸ“§ãƒ¡ãƒ¼ãƒ«ã‚ã‚Š": download_email,
            "ğŸ“TELã‚ã‚Š": download_tel,
            "ğŸŒURLã‚ã‚Š": download_url
        }
    
    # æ›´æ–°ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    st.markdown("### ğŸ“… æ›´æ–°ãƒ¢ãƒ¼ãƒ‰")
    col1, col2 = st.columns(2)
    with col1:
        update_mode = st.radio(
            "å–å¾—ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«",
            ["ä»Šæœˆã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿", "æœŸé–“ã‚’æŒ‡å®š", "å…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå®Œå…¨æ›´æ–°ï¼‰"],
            help="æœˆæ¬¡é‹ç”¨ã§ã¯ã€Œä»Šæœˆã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã€ã‚’æ¨å¥¨"
        )
    with col2:
        if update_mode == "ä»Šæœˆã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿":
            st.info("ğŸ“… æœ€çµ‚æ›´æ–°æ—¥æ™‚ãŒä»Šæœˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å–å¾—ã—ã¾ã™")
        elif update_mode == "æœŸé–“ã‚’æŒ‡å®š":
            st.info("ğŸ“… æŒ‡å®šã—ãŸæœŸé–“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¾ã™")
        else:
            st.warning("âš ï¸ å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¾ã™ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")
    
    # æœŸé–“æŒ‡å®šã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    if update_mode == "æœŸé–“ã‚’æŒ‡å®š":
        st.markdown("### ğŸ“… æœŸé–“æŒ‡å®š")
        col1, col2 = st.columns(2)
        with col1:
            from datetime import datetime, timedelta
            start_date = st.date_input(
                "é–‹å§‹æ—¥",
                value=datetime.now() - timedelta(days=30),
                help="å–å¾—ã™ã‚‹æœŸé–“ã®é–‹å§‹æ—¥"
            )
        with col2:
            end_date = st.date_input(
                "çµ‚äº†æ—¥",
                value=datetime.now(),
                help="å–å¾—ã™ã‚‹æœŸé–“ã®çµ‚äº†æ—¥"
            )
    
    if st.button("å¯¾è±¡ä»¶æ•°ã‚’ç¢ºèª", disabled=not (notion_api_key and database_id)):
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å ´åˆã€é¸æŠã•ã‚Œã¦ã„ãªã„ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚‹ã‹ç¢ºèª
        if filter_option == "ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡":
            selected_categories = st.session_state.get('selected_categories', {})
            selected_count = sum(selected_categories.values())
            
            if selected_count == 0:
                st.error("âŒ å°‘ãªãã¨ã‚‚1ã¤ã®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„")
                return
            
            not_selected = [cat for cat, selected in selected_categories.items() if not selected]
            if not_selected:
                st.info(f"â„¹ï¸ ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã›ã‚“: {', '.join(not_selected)}")
        
        try:
            notion = Client(
                auth=notion_api_key,
                notion_version="2022-06-28"
            )
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’ä½œæˆ
            def create_filter_conditions(filter_option, update_mode, start_date=None, end_date=None):
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                date_filter = None
                if update_mode == "ä»Šæœˆã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿":
                    from datetime import datetime
                    
                    now = datetime.now()
                    first_day = datetime(now.year, now.month, 1)
                    first_day_iso = first_day.isoformat() + "Z"
                    
                    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«æœŸé–“ã‚’è¡¨ç¤º
                    st.info(f"ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æœŸé–“: {now.strftime('%Yå¹´%mæœˆ1æ—¥')} 00:00:00 ä»¥é™")
                    st.code(f"APIæ¡ä»¶: last_edited_time >= {first_day_iso}", language="json")
                    
                    date_filter = {
                        "property": "Last edited time", 
                        "last_edited_time": {
                            "on_or_after": first_day_iso
                        }
                    }
                elif update_mode == "æœŸé–“ã‚’æŒ‡å®š" and start_date and end_date:
                    from datetime import datetime, time
                    
                    # é–‹å§‹æ—¥ã®00:00:00
                    start_datetime = datetime.combine(start_date, time.min)
                    start_iso = start_datetime.isoformat() + "Z"
                    
                    # çµ‚äº†æ—¥ã®23:59:59
                    end_datetime = datetime.combine(end_date, time.max)
                    end_iso = end_datetime.isoformat() + "Z"
                    
                    st.info(f"ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æœŸé–“: {start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} ã€œ {end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
                    st.code(f"APIæ¡ä»¶: {start_iso} <= last_edited_time <= {end_iso}", language="json")
                    
                    date_filter = {
                        "and": [
                            {
                                "property": "Last edited time",
                                "last_edited_time": {
                                    "on_or_after": start_iso
                                }
                            },
                            {
                                "property": "Last edited time",
                                "last_edited_time": {
                                    "on_or_before": end_iso
                                }
                            }
                        ]
                    }
                
                if filter_option == "ã™ã¹ã¦çµ±åˆ":
                    # å¾“æ¥ã®ã™ã¹ã¦çµ±åˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    base_filter = {
                        "and": [
                            {
                                "or": [
                                    {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ã‚ã‚Š"}},
                                    {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "TELã¨URL"}},
                                    {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åãƒ»ä½æ‰€ãƒ»URL"}},
                                    {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "Telã€ä½æ‰€ã€URL"}},
                                    {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åã¨URLï¼ˆç›´ã§ä¼æ¥­HPãƒªãƒ³ã‚¯ï¼‰"}},
                                    {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åã¨URLã®ã¿"}}
                                ]
                            },
                            {"property": "ãƒ•ã‚¡ã‚¤ãƒ«", "files": {"is_not_empty": True}}
                        ]
                    }
                    if date_filter:
                        base_filter["and"].append(date_filter)
                    
                    return {"ã™ã¹ã¦": base_filter}
                
                else:  # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡
                    filters = {}
                    selected_categories = st.session_state.get('selected_categories', {
                        "ğŸ“§ãƒ¡ãƒ¼ãƒ«ã‚ã‚Š": True,
                        "ğŸ“TELã‚ã‚Š": True,
                        "ğŸŒURLã‚ã‚Š": True
                    })
                    
                    # 1. ãƒ¡ãƒ¼ãƒ«ã‚ã‚Šãƒ•ã‚©ãƒ«ãƒ€
                    if selected_categories.get("ğŸ“§ãƒ¡ãƒ¼ãƒ«ã‚ã‚Š", True):
                        email_filter = {
                            "and": [
                                {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ã‚ã‚Š"}},
                                {"property": "ãƒ•ã‚¡ã‚¤ãƒ«", "files": {"is_not_empty": True}}
                            ]
                        }
                        if date_filter:
                            email_filter["and"].append(date_filter)
                        filters["ğŸ“§ãƒ¡ãƒ¼ãƒ«ã‚ã‚Š"] = email_filter
                    
                    # 2. TELã‚ã‚Šãƒ•ã‚©ãƒ«ãƒ€
                    if selected_categories.get("ğŸ“TELã‚ã‚Š", True):
                        tel_filter = {
                            "and": [
                                {
                                    "or": [
                                        {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "TELã¨URL"}},
                                        {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "Telã€ä½æ‰€ã€URL"}}
                                    ]
                                },
                                {"property": "ãƒ•ã‚¡ã‚¤ãƒ«", "files": {"is_not_empty": True}}
                            ]
                        }
                        if date_filter:
                            tel_filter["and"].append(date_filter)
                        filters["ğŸ“TELã‚ã‚Š"] = tel_filter
                    
                    # 3. URLã‚ã‚Šãƒ•ã‚©ãƒ«ãƒ€
                    if selected_categories.get("ğŸŒURLã‚ã‚Š", True):
                        url_filter = {
                            "and": [
                                {
                                    "or": [
                                        {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åãƒ»ä½æ‰€ãƒ»URL"}},
                                        {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åã¨URLï¼ˆç›´ã§ä¼æ¥­HPãƒªãƒ³ã‚¯ï¼‰"}},
                                        {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åã¨URLã®ã¿"}}
                                    ]
                                },
                                {"property": "ãƒ•ã‚¡ã‚¤ãƒ«", "files": {"is_not_empty": True}}
                            ]
                        }
                        if date_filter:
                            url_filter["and"].append(date_filter)
                        filters["ğŸŒURLã‚ã‚Š"] = url_filter
                    
                    return filters
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’å–å¾—
            if update_mode == "æœŸé–“ã‚’æŒ‡å®š":
                filter_conditions = create_filter_conditions(filter_option, update_mode, start_date, end_date)
            else:
                filter_conditions = create_filter_conditions(filter_option, update_mode)
            
            if update_mode == "ä»Šæœˆã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿":
                from datetime import datetime
                now = datetime.now()
                st.info(f"ğŸ“… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶: {now.strftime('%Yå¹´%mæœˆ')}ã®æœ€çµ‚æ›´æ–°æ—¥æ™‚")
                st.success("âœ… æœ€çµ‚æ›´æ–°æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨ï¼ˆä»Šæœˆç·¨é›†ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ï¼‰")
            
            with st.spinner("å¯¾è±¡ä»¶æ•°ã‚’ç¢ºèªä¸­..."):
                total_counts = {}
                
                for category, filter_condition in filter_conditions.items():
                    # ä»¶æ•°ã‚’å–å¾—
                    count = 0
                    start_cursor = None
                    
                    while True:
                        response = notion.databases.query(
                            database_id=database_id,
                            filter=filter_condition,
                            start_cursor=start_cursor,
                            page_size=100
                        )
                        items = response.get("results", [])
                        count += len(items)
                        
                        if not response.get("has_more"):
                            break
                        start_cursor = response.get("next_cursor")
                    
                    total_counts[category] = count
                
                # çµæœè¡¨ç¤º
                if filter_option == "ã™ã¹ã¦çµ±åˆ":
                    total = total_counts["ã™ã¹ã¦"]
                    st.success(f"ğŸ¯ **å¯¾è±¡ã‚¢ã‚¤ãƒ†ãƒ : {total}ä»¶**")
                else:
                    st.success("ğŸ¯ **ã‚«ãƒ†ã‚´ãƒªåˆ¥å¯¾è±¡ä»¶æ•°:**")
                    col1, col2, col3 = st.columns(3)
                    
                    categories = list(total_counts.keys())
                    with col1:
                        if len(categories) > 0:
                            st.metric(categories[0], total_counts[categories[0]])
                    with col2:
                        if len(categories) > 1:
                            st.metric(categories[1], total_counts[categories[1]])
                    with col3:
                        if len(categories) > 2:
                            st.metric(categories[2], total_counts[categories[2]])
                    
                    total = sum(total_counts.values())
                    st.info(f"**åˆè¨ˆ: {total}ä»¶**")
                
                # æœŸé–“è¡¨ç¤º
                if update_mode == "ä»Šæœˆã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿":
                    st.info(f"ğŸ“… æœŸé–“: {now.strftime('%Yå¹´%mæœˆ')}ã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ã«è©²å½“ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«")
                elif update_mode == "æœŸé–“ã‚’æŒ‡å®š":
                    st.info(f"ğŸ“… æœŸé–“: {start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} ã€œ {end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
                else:
                    st.info("ğŸ“… æœŸé–“: å…¨æœŸé–“")
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                st.session_state.target_counts = total_counts
                st.session_state.target_filters = filter_conditions
                st.session_state.target_mode = update_mode
                st.session_state.target_filter_option = filter_option
                st.session_state.google_api_key = google_api_key
                st.session_state.fallback_option = fallback_option
                
                if total > 0:
                    st.warning("âš ï¸ å¯¾è±¡ä»¶æ•°ã‚’ç¢ºèªå¾Œã€ä¸‹ã®ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
                else:
                    st.error("âŒ å¯¾è±¡ã¨ãªã‚‹ã‚¢ã‚¤ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # å¯¾è±¡ä»¶æ•°ç¢ºèªå¾Œã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    if 'target_counts' in st.session_state and sum(st.session_state.target_counts.values()) > 0:
        st.markdown("---")
        st.markdown("### ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ")
        
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"ğŸ¯ **å¯¾è±¡: {sum(st.session_state.target_counts.values())}ä»¶**")
            st.info(f"ğŸ“… **ãƒ¢ãƒ¼ãƒ‰: {st.session_state.target_mode}**")
        
        with col2:
            if st.button("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ", type="primary"):
                try:
                    notion = Client(
                        auth=notion_api_key,
                        notion_version="2022-06-28"
                    )
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰è¨­å®šã‚’å–å¾—
                    google_api_key = st.session_state.get('google_api_key', '')
                    fallback_option = st.session_state.get('fallback_option', 'ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ')
                    
                    with st.spinner("Notionã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                        # ã‚«ãƒ†ã‚´ãƒªåˆ¥å‡¦ç†
                        if st.session_state.target_filter_option == "ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡":
                            categorized_files = {}
                            
                            for category, filter_condition in st.session_state.target_filters.items():
                                all_items = []
                                start_cursor = None
                                
                                while True:
                                    response = notion.databases.query(
                                        database_id=database_id,
                                        filter=filter_condition,
                                        start_cursor=start_cursor
                                    )
                                    items = response.get("results", [])
                                    all_items.extend(items)
                                    if not response.get("has_more"):
                                        break
                                    start_cursor = response.get("next_cursor")
                                
                                st.info(f"{category}: {len(all_items)}ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—")
                                
                                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†
                                downloaded_files = []
                                headers = {
                                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                                }
                                
                                for item_idx, item in enumerate(all_items):
                                    properties = item["properties"]
                                    
                                    # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                                    page_title = "untitled"
                                    if "åå‰" in properties and properties["åå‰"]["title"]:
                                        page_title = properties["åå‰"]["title"][0]["plain_text"] if properties["åå‰"]["title"] else "untitled"
                                    elif "Name" in properties and properties["Name"]["title"]:
                                        page_title = properties["Name"]["title"][0]["plain_text"] if properties["Name"]["title"] else "untitled"
                                    
                                    file_property = properties.get("ãƒ•ã‚¡ã‚¤ãƒ«")
                                    if file_property and file_property["type"] == "files":
                                        for file_idx, file_info in enumerate(file_property["files"]):
                                            try:
                                                # Notionã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
                                                if file_info["type"] == "file":
                                                    file_name = file_info["name"]
                                                    file_url = file_info["file"]["url"]
                                                    ext = os.path.splitext(file_name)[1].lower()
                                                    
                                                    if ext in [".csv", ".xlsx", ".xls"]:
                                                        response = requests.get(file_url, headers=headers)
                                                        response.raise_for_status()
                                                        final_name = f"{os.path.splitext(file_name)[0]}_{item_idx+1}_{file_idx+1}{ext}"
                                                        downloaded_files.append((final_name, response.content))
                                                
                                                # Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç­‰ã®å¤–éƒ¨URL
                                                elif file_info["type"] == "external":
                                                    url = file_info["external"]["url"]
                                                    if is_google_sheet_url(url):
                                                        success = False
                                                        
                                                        # æ–¹æ³•1: Google Sheets APIã‚’è©¦è¡Œ
                                                        if google_api_key:
                                                            st.info(f"ğŸ”‘ Google Sheets APIã§ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                                                            csv_data = google_sheet_to_csv_with_api(url, google_api_key)
                                                            if csv_data:
                                                                sheet_id = extract_sheet_id(url)
                                                                file_name = f"{page_title}_{sheet_id}_{item_idx+1}_{file_idx+1}.csv"
                                                                downloaded_files.append((file_name, csv_data))
                                                                success = True
                                                                st.success(f"âœ… APIã§å–å¾—æˆåŠŸ: {file_name}")
                                                        
                                                        # æ–¹æ³•2: å…¬é–‹URLã‚’è©¦è¡Œ
                                                        if not success:
                                                            st.info(f"ğŸŒ å…¬é–‹URLã§ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                                                            csv_url = google_sheet_to_csv_url(url)
                                                            if csv_url:
                                                                try:
                                                                    response = requests.get(csv_url, headers=headers)
                                                                    response.raise_for_status()
                                                                    sheet_id = extract_sheet_id(url)
                                                                    file_name = f"{page_title}_{sheet_id}_{item_idx+1}_{file_idx+1}.csv"
                                                                    downloaded_files.append((file_name, response.content))
                                                                    success = True
                                                                    st.success(f"âœ… å…¬é–‹URLã§å–å¾—æˆåŠŸ: {file_name}")
                                                                except requests.exceptions.HTTPError as e:
                                                                    if fallback_option == "ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ":
                                                                        st.warning(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {url} - {e}")
                                                                    else:
                                                                        raise e
                                                        
                                                        if not success and fallback_option == "ã‚¨ãƒ©ãƒ¼ã§åœæ­¢":
                                                            raise Exception(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {url}")
                                                        elif not success:
                                                            st.warning(f"âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {url}")
                                                        
                                            except Exception as e:
                                                if fallback_option == "ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ":
                                                    st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")
                                                else:
                                                    st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                
                                categorized_files[category] = downloaded_files
                            
                            st.session_state.notion_files_categorized = categorized_files
                            st.session_state.notion_update_mode = st.session_state.target_mode
                            
                            # çµæœè¡¨ç¤º
                            st.success("âœ… ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                            for category, files in categorized_files.items():
                                st.info(f"{category}: {len(files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
                        
                        else:  # ã™ã¹ã¦çµ±åˆ
                            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’ä½¿ç”¨
                            all_items = []
                            start_cursor = None
                            
                            while True:
                                response = notion.databases.query(
                                    database_id=database_id,
                                    filter=st.session_state.target_filters["ã™ã¹ã¦"],
                                    start_cursor=start_cursor
                                )
                                items = response.get("results", [])
                                all_items.extend(items)
                                if not response.get("has_more"):
                                    break
                                start_cursor = response.get("next_cursor")
                            
                            st.success(f"ğŸ¯ {len(all_items)}ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ã—ã¾ã—ãŸ")
                            
                            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†
                            downloaded_files = []
                            failed_files = []
                            progress_bar = st.progress(0)
                            
                            headers = {
                                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                            }
                            
                            for item_idx, item in enumerate(all_items):
                                properties = item["properties"]
                                
                                # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                                page_title = "untitled"
                                if "åå‰" in properties and properties["åå‰"]["title"]:
                                    page_title = properties["åå‰"]["title"][0]["plain_text"] if properties["åå‰"]["title"] else "untitled"
                                elif "Name" in properties and properties["Name"]["title"]:
                                    page_title = properties["Name"]["title"][0]["plain_text"] if properties["Name"]["title"] else "untitled"
                                
                                file_property = properties.get("ãƒ•ã‚¡ã‚¤ãƒ«")
                                if file_property and file_property["type"] == "files":
                                    for file_idx, file_info in enumerate(file_property["files"]):
                                        try:
                                            # Notionã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
                                            if file_info["type"] == "file":
                                                file_name = file_info["name"]
                                                file_url = file_info["file"]["url"]
                                                ext = os.path.splitext(file_name)[1].lower()
                                                
                                                if ext in [".csv", ".xlsx", ".xls"]:
                                                    response = requests.get(file_url, headers=headers)
                                                    response.raise_for_status()
                                                    final_name = f"{os.path.splitext(file_name)[0]}_{item_idx+1}_{file_idx+1}{ext}"
                                                    downloaded_files.append((final_name, response.content))
                                            
                                            # Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç­‰ã®å¤–éƒ¨URL
                                            elif file_info["type"] == "external":
                                                url = file_info["external"]["url"]
                                                if is_google_sheet_url(url):
                                                    success = False
                                                    
                                                    # æ–¹æ³•1: Google Sheets APIã‚’è©¦è¡Œ
                                                    if google_api_key:
                                                        csv_data = google_sheet_to_csv_with_api(url, google_api_key)
                                                        if csv_data:
                                                            sheet_id = extract_sheet_id(url)
                                                            file_name = f"{page_title}_{sheet_id}_{item_idx+1}_{file_idx+1}.csv"
                                                            downloaded_files.append((file_name, csv_data))
                                                            success = True
                                                    
                                                    # æ–¹æ³•2: å…¬é–‹URLã‚’è©¦è¡Œ
                                                    if not success:
                                                        csv_url = google_sheet_to_csv_url(url)
                                                        if csv_url:
                                                            try:
                                                                response = requests.get(csv_url, headers=headers)
                                                                response.raise_for_status()
                                                                sheet_id = extract_sheet_id(url)
                                                                file_name = f"{page_title}_{sheet_id}_{item_idx+1}_{file_idx+1}.csv"
                                                                downloaded_files.append((file_name, response.content))
                                                                success = True
                                                            except requests.exceptions.HTTPError:
                                                                if fallback_option == "ã‚¨ãƒ©ãƒ¼ã§åœæ­¢":
                                                                    raise
                                                    
                                                    if not success and fallback_option == "ã‚¨ãƒ©ãƒ¼ã§åœæ­¢":
                                                        raise Exception(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {url}")
                                                    
                                        except Exception as e:
                                            if fallback_option == "ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ":
                                                failed_files.append(f"ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")
                                            else:
                                                failed_files.append(f"ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                                
                                progress_bar.progress((item_idx + 1) / len(all_items))
                            
                            st.session_state.notion_files = downloaded_files
                            st.session_state.notion_update_mode = st.session_state.target_mode
                            
                            # çµæœè¡¨ç¤º
                            col1, col2 = st.columns(2)
                            with col1:
                                st.success(f"âœ… {len(downloaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
                            with col2:
                                if failed_files:
                                    st.warning(f"âš ï¸ {len(failed_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ")
                            
                            if failed_files:
                                with st.expander("âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°"):
                                    for error in failed_files:
                                        st.write(f"- {error}")
                        
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
                        keys_to_delete = ['target_counts', 'target_filters', 'target_mode', 
                                        'target_filter_option', 'google_api_key', 'fallback_option']
                        for key in keys_to_delete:
                            if key in st.session_state:
                                del st.session_state[key]
                        
                except Exception as e:
                    st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def file_upload_processing():
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»çµ±åˆå‡¦ç†")
    
    # Notionã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
    if 'notion_files' in st.session_state and st.session_state.notion_files:
        update_mode = st.session_state.get('notion_update_mode', 'ä¸æ˜')
        st.info(f"ğŸ’¾ Notionã‹ã‚‰{len(st.session_state.notion_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼ˆ{update_mode}ï¼‰")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("ğŸ“¥ Notionãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"):
                process_files(st.session_state.notion_files, "Notion")
        with col2:
            merge_option = st.checkbox(
                "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ", 
                value=True, 
                help="ãƒã‚§ãƒƒã‚¯æ™‚ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆã—é‡è¤‡å‰Šé™¤\næœªãƒã‚§ãƒƒã‚¯æ™‚ï¼šæ–°è¦ãƒ‡ãƒ¼ã‚¿ã§å®Œå…¨ç½®æ›"
            )
            st.session_state.merge_with_existing = merge_option
        with col3:
            if st.button("ğŸ—‘ï¸ Notionãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢"):
                del st.session_state.notion_files
                if 'notion_update_mode' in st.session_state:
                    del st.session_state.notion_update_mode
                st.rerun()
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
    if 'notion_files_categorized' in st.session_state and st.session_state.notion_files_categorized:
        st.info("ğŸ’¾ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™")
        
        for category, files in st.session_state.notion_files_categorized.items():
            if files:
                st.markdown(f"#### {category} ({len(files)}å€‹)")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button(f"ğŸ“¥ {category}ã‚’å‡¦ç†", key=f"process_{category}"):
                        process_files(files, f"{category}")
                
                with col2:
                    merge_option = st.checkbox(
                        "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ", 
                        value=True, 
                        key=f"merge_{category}",
                        help="ãƒã‚§ãƒƒã‚¯æ™‚ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆã—é‡è¤‡å‰Šé™¤\næœªãƒã‚§ãƒƒã‚¯æ™‚ï¼šæ–°è¦ãƒ‡ãƒ¼ã‚¿ã§å®Œå…¨ç½®æ›"
                    )
                    st.session_state[f'merge_with_existing_{category}'] = merge_option
                
                with col3:
                    if st.button(f"ğŸ—‘ï¸ {category}ã‚’ã‚¯ãƒªã‚¢", key=f"clear_{category}"):
                        del st.session_state.notion_files_categorized[category]
                        st.rerun()
        
        # å…¨ã‚«ãƒ†ã‚´ãƒªã‚¯ãƒªã‚¢
        if st.button("ğŸ—‘ï¸ å…¨ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¯ãƒªã‚¢"):
            del st.session_state.notion_files_categorized
            if 'notion_update_mode' in st.session_state:
                del st.session_state.notion_update_mode
            st.rerun()
    
    uploaded_files = st.file_uploader(
        "CSVã¾ãŸã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.write(f"ğŸ“‚ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(uploaded_files)}")
        
        if st.button("ğŸ”„ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"):
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¤‰æ›
            file_data = []
            for uploaded_file in uploaded_files:
                content = uploaded_file.read()
                uploaded_file.seek(0)  # ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                file_data.append((uploaded_file.name, content))
            
            process_files(file_data, "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

def process_files(file_data, source_type):
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ï¼ˆè»½é‡åŒ–ç‰ˆï¼‰"""
    processed_dfs = []
    error_files = []
    total_stats = {"email_extracted": 0, "tel_extracted": 0, "files_processed": 0}
    
    # å¤§é‡å‡¦ç†ç”¨ã®è¨­å®š
    total_files = len(file_data)
    batch_size = 25  # 25ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒãƒƒãƒå‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„ï¼‰
    is_large_batch = total_files > 20
    
    # 700ä»¶ä»¥ä¸Šã®å¤§é‡å‡¦ç†å¯¾å¿œ
    if total_files > 700:
        batch_size = 15  # ã•ã‚‰ã«å°ã•ã„ãƒãƒƒãƒã«
        st.warning(f"""
        âš ï¸ å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆ{total_files}ä»¶ï¼‰
        - å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™
        - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–ã—ã¾ã™
        - é€”ä¸­çµŒéãŒè¡¨ç¤ºã•ã‚Œã¾ã™
        """)
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    progress_bar = st.progress(0)
    status_container = st.empty()
    
    if is_large_batch:
        st.info(f"ğŸ”„ å¤§é‡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼š{total_files}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
        debug_mode = st.checkbox("è©³ç´°ãƒ‡ãƒãƒƒã‚°ã‚’è¡¨ç¤º", value=False, 
                                help="å¤§é‡å‡¦ç†æ™‚ã¯ç„¡åŠ¹ã‚’æ¨å¥¨ï¼ˆå‡¦ç†é€Ÿåº¦å‘ä¸Šï¼‰")
    else:
        debug_mode = True
    
    # ãƒãƒƒãƒå‡¦ç†
    for batch_start in range(0, total_files, batch_size):
        batch_end = min(batch_start + batch_size, total_files)
        batch_files = file_data[batch_start:batch_end]
        
        if is_large_batch:
            status_container.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_start//batch_size + 1}/{(total_files-1)//batch_size + 1} å‡¦ç†ä¸­ ({batch_start+1}-{batch_end}ä»¶ç›®)")
        
        batch_processed = []
        batch_errors = []
        
        for idx, (filename, content) in enumerate(batch_files):
            global_idx = batch_start + idx
            
            if not is_large_batch or debug_mode:
                status_container.text(f"å‡¦ç†ä¸­: {filename} ({global_idx+1}/{total_files})")
            
            try:
                # è»½é‡åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                df = process_single_file_lightweight(filename, content, debug_mode)
                
                if df is not None:
                    # è»½é‡åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‡¦ç†
                    processed_df, stats, error = process_dataframe_lightweight(df, filename, debug_mode)
                    
                    if processed_df is not None:
                        batch_processed.append(processed_df)
                        total_stats["email_extracted"] += stats["email_extracted"]
                        total_stats["tel_extracted"] += stats["tel_extracted"]
                        total_stats["files_processed"] += 1
                        
                        # æˆåŠŸãƒ­ã‚°ï¼ˆç°¡æ½”ï¼‰
                        if not is_large_batch or debug_mode:
                            st.success(f"âœ… {filename}: {len(processed_df)}è¡Œå‡¦ç†å®Œäº†")
                    else:
                        batch_errors.append((filename, error))
                        if not is_large_batch or debug_mode:
                            st.error(f"âŒ {filename}: {error}")
                else:
                    batch_errors.append((filename, "ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—"))
                    
            except Exception as e:
                batch_errors.append((filename, str(e)))
                if not is_large_batch or debug_mode:
                    st.error(f"âŒ {filename}: {e}")
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            progress_bar.progress((global_idx + 1) / total_files)
        
        # ãƒãƒƒãƒçµæœã‚’ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã«è¿½åŠ 
        processed_dfs.extend(batch_processed)
        error_files.extend(batch_errors)
        
        # å¤§é‡å‡¦ç†æ™‚ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if is_large_batch and len(processed_dfs) > 100:
            import gc
            gc.collect()
        
        # ãƒãƒƒãƒå®Œäº†é€šçŸ¥
        if is_large_batch:
            success_count = len(batch_processed)
            error_count = len(batch_errors)
            st.info(f"ğŸ“¦ ãƒãƒƒãƒ {batch_start//batch_size + 1} å®Œäº†: æˆåŠŸ{success_count}ä»¶, ã‚¨ãƒ©ãƒ¼{error_count}ä»¶")
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆè»½é‡åŒ–ãƒ»å®‰å…¨åŒ–ï¼‰
    if processed_dfs:
        status_container.info("ğŸ”— ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
        
        # Step 1: å„DataFrameã®å‰å‡¦ç†ï¼ˆå®‰å…¨åŒ–ï¼‰
        safe_dfs = []
        for i, df in enumerate(processed_dfs):
            try:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ˜ç¤ºçš„ã«ãƒªã‚»ãƒƒãƒˆ
                df = df.reset_index(drop=True)
                
                # åˆ—åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£
                columns = df.columns.tolist()
                seen_columns = {}
                new_columns = []
                
                for col in columns:
                    col_str = str(col)
                    if col_str in seen_columns:
                        seen_columns[col_str] += 1
                        new_col_name = f"{col_str}_dup{seen_columns[col_str]}"
                        new_columns.append(new_col_name)
                        if debug_mode:
                            st.warning(f"âš ï¸ DataFrame{i}: é‡è¤‡åˆ—åä¿®æ­£ '{col_str}' â†’ '{new_col_name}'")
                    else:
                        seen_columns[col_str] = 0
                        new_columns.append(col_str)
                
                df.columns = new_columns
                
                # åˆ—åã‚’æ–‡å­—åˆ—ã«çµ±ä¸€
                df.columns = [str(col) for col in df.columns]
                
                safe_dfs.append(df)
                
            except Exception as e:
                if debug_mode:
                    st.error(f"âŒ DataFrame{i}ã®å‰å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not safe_dfs:
            st.error("âŒ æœ‰åŠ¹ãªDataFrameãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # Step 2: å®‰å…¨ãªçµåˆå‡¦ç†
        try:
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªçµ±åˆ
            if len(safe_dfs) > 100:
                # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€æ®µéšçš„ã«çµ±åˆ
                st.info("ğŸ“Š å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’æ®µéšçš„ã«çµ±åˆä¸­...")
                # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«å¿œã˜ãŸãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºèª¿æ•´
                if len(safe_dfs) > 700:
                    chunk_size = 15  # å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«æ™‚ã¯å°ã•ã„ãƒãƒ£ãƒ³ã‚¯
                elif len(safe_dfs) > 300:
                    chunk_size = 25
                else:
                    chunk_size = 50
                
                merged_chunks = []
                
                for i in range(0, len(safe_dfs), chunk_size):
                    chunk = safe_dfs[i:i+chunk_size]
                    
                    # safe_concat_dataframesé–¢æ•°ã‚’ä½¿ç”¨
                    merged_chunk = safe_concat_dataframes(chunk, debug_mode)
                    
                    merged_chunks.append(merged_chunk)
                    
                    if i % (chunk_size * 5) == 0:  # 5ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«é€²æ—è¡¨ç¤º
                        progress_pct = min((i+chunk_size) / len(safe_dfs) * 100, 100)
                        st.info(f"çµ±åˆé€²æ—: {min(i+chunk_size, len(safe_dfs))}/{len(safe_dfs)} ãƒãƒ£ãƒ³ã‚¯ ({progress_pct:.1f}%)")
                        
                        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ï¼ˆ700ä»¶ä»¥ä¸Šã®å ´åˆï¼‰
                        if len(safe_dfs) > 700 and i % (chunk_size * 10) == 0:
                            import gc
                            gc.collect()
                
                # æœ€çµ‚çµ±åˆã‚‚åŒæ§˜ã«ä¿®æ­£
                if len(merged_chunks) == 1:
                    merged_df = merged_chunks[0]
                else:
                    aligned_chunks = align_dataframe_columns(merged_chunks, debug_mode)
                    merged_df = safe_concat_dataframes(aligned_chunks, debug_mode)
            else:
                # é€šå¸¸ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
                if len(safe_dfs) == 1:
                    merged_df = safe_dfs[0].copy()
                else:
                    # åˆ—åã‚’çµ±ä¸€ã—ã¦ã‹ã‚‰çµåˆ
                    aligned_dfs = align_dataframe_columns(safe_dfs, debug_mode)
                    merged_df = safe_concat_dataframes(aligned_dfs, debug_mode)
            
            # æœ€çµ‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
            merged_df = merged_df.reset_index(drop=True)
            
        except Exception as e:
            st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ğŸ”„ ä»£æ›¿æ–¹æ³•ã§çµ±åˆã‚’è©¦è¡Œä¸­...")
            
            # ä»£æ›¿çµ±åˆæ–¹æ³•
            try:
                merged_df = concatenate_dataframes_safely(safe_dfs, debug_mode)
            except Exception as e2:
                st.error(f"âŒ ä»£æ›¿çµ±åˆã‚‚å¤±æ•—: {e2}")
                return
        
        # è»½é‡åŒ–ã•ã‚ŒãŸé‡è¤‡å‰Šé™¤
        merged_df = remove_duplicates_lightweight(merged_df, is_large_batch)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ
        if not st.session_state.merged_data.empty and st.session_state.get('merge_with_existing', True):
            status_container.info("ğŸ”„ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆä¸­...")
            combined_df = pd.concat([st.session_state.merged_data, merged_df], ignore_index=True)
            combined_df = remove_duplicates_lightweight(combined_df, True)
            st.session_state.merged_data = combined_df
        else:
            st.session_state.merged_data = merged_df
        
        # çµ±è¨ˆæƒ…å ±ä¿å­˜
        st.session_state.processing_stats = total_stats
        
        # ç°¡æ½”ãªçµæœè¡¨ç¤º
        st.success(f"""
        âœ… **{source_type}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†**
        - å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_stats['files_processed']}/{total_files}å€‹
        - æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ•°: {len(st.session_state.merged_data)}ä»¶
        - ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(error_files)}å€‹
        """)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤ºï¼ˆç°¡æ½”ï¼‰
    if error_files:
        with st.expander(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ({len(error_files)}ä»¶)"):
            for filename, error in error_files:
                st.write(f"- **{filename}**: {error}")
    
    status_container.empty()
    progress_bar.empty()

def process_single_file_lightweight(filename, content, debug_mode=False):
    """è»½é‡åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    try:
        if filename.lower().endswith('.csv'):
            # æ—¥æœ¬èªCSVç”¨ã®è»½é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
            encodings_to_try = ['utf-8-sig', 'utf-8', 'shift_jis', 'cp932']
            detected_encoding = detect(content).get("encoding", "utf-8")
            
            if detected_encoding and detected_encoding not in encodings_to_try:
                encodings_to_try.insert(0, detected_encoding)
            
            for encoding in encodings_to_try:
                try:
                    content_str = content.decode(encoding)
                    if content_str.startswith('\ufeff'):
                        content_str = content_str[1:]
                    
                    df = pd.read_csv(io.StringIO(content_str), dtype=str, on_bad_lines="skip")
                    df = df.reset_index(drop=True)  # è¿½åŠ ï¼šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
                    
                    # ç°¡å˜ãªæ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯
                    first_col = str(df.columns[0]) if len(df.columns) > 0 else ""
                    if not any(suspect in first_col for suspect in ['éŒ²éŸ³', 'å¢¨è¨‚', 'éœ‡ç½']):
                        if debug_mode:
                            st.info(f"âœ… {encoding}ã§èª­ã¿è¾¼ã¿æˆåŠŸ: {filename}")
                        return df
                except:
                    continue
            
            # å…¨ã¦å¤±æ•—ã—ãŸå ´åˆã€å¼·åˆ¶ãƒ‡ã‚³ãƒ¼ãƒ‰
            content_str = content.decode(detected_encoding, errors='ignore')
            df = pd.read_csv(io.StringIO(content_str), dtype=str, on_bad_lines="skip")
            df = df.reset_index(drop=True)  # è¿½åŠ ï¼šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
            
            # æ–‡å­—åŒ–ã‘è‡ªå‹•ä¿®æ­£
            if any(any(suspect in str(col) for suspect in ['éŒ²éŸ³', 'å¢¨è¨‚', 'éœ‡ç½']) for col in df.columns):
                expected_columns = ['å±•ç¤ºä¼šå', 'æ¥­ç¨®', 'å±•ç¤ºä¼šåˆæ—¥', 'å±•ç¤ºä¼šæœ€çµ‚æ—¥', 'ä¼šç¤¾åã‚¿ã‚¤ãƒˆãƒ«', 'ä½æ‰€', 'TEL', 'URL', 'ä¼šç¤¾å']
                if len(df.columns) == len(expected_columns):
                    df.columns = expected_columns
                    df = df.reset_index(drop=True)  # è¿½åŠ ï¼šåˆ—åå¤‰æ›´å¾Œã‚‚ãƒªã‚»ãƒƒãƒˆ
                    if debug_mode:
                        st.warning(f"âš ï¸ æ–‡å­—åŒ–ã‘ä¿®æ­£: {filename}")
            
            return df
            
        else:
            # Excelãƒ•ã‚¡ã‚¤ãƒ«
            df = pd.read_excel(io.BytesIO(content), dtype=str, engine='openpyxl')
            return df.reset_index(drop=True)  # è¿½åŠ ï¼šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
            
    except Exception as e:
        if debug_mode:
            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filename} - {e}")
        return None

def process_dataframe_lightweight(df, filename, debug_mode=False):
    """è»½é‡åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†"""
    try:
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼šãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        if len(df) > 10000:
            # å¤§ãã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆã€ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
            df = df.copy()
            empty_cols = [col for col in df.columns if df[col].isna().all()]
            if empty_cols:
                df = df.drop(columns=empty_cols)
        
        stats = {"email_extracted": 0, "tel_extracted": 0}
        inferred_event_name = os.path.splitext(filename)[0]
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ˜ç¤ºçš„ã«ãƒªã‚»ãƒƒãƒˆï¼ˆé‡è¦ï¼‰
        df = df.reset_index(drop=True)
        
        # é‡è¤‡åˆ—åä¿®æ­£ï¼ˆç°¡æ½”ç‰ˆï¼‰
        original_columns = list(df.columns)
        seen_columns = {}
        new_columns = []
        
        for col in original_columns:
            col_str = str(col).strip()
            if col_str in seen_columns:
                seen_columns[col_str] += 1
                new_col_name = f"{col_str}_dup{seen_columns[col_str]}"
                new_columns.append(new_col_name)
                if debug_mode:
                    st.warning(f"âš ï¸ é‡è¤‡åˆ—åä¿®æ­£: '{col_str}' â†’ '{new_col_name}'")
            else:
                seen_columns[col_str] = 0
                new_columns.append(col_str)
        
        df.columns = new_columns
        
        # åˆ—åã‚’æ–‡å­—åˆ—ã«çµ±ä¸€
        df.columns = [str(col) for col in df.columns]
        
        # å†åº¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
        df = df.reset_index(drop=True)
        
        # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
        columns_to_drop = []
        for col in df.columns:
            col_lower = str(col).lower().strip()
            # ãƒ–ãƒ¼ã‚¹ç•ªå·ã€å°é–“ç•ªå·ã€ãƒ­ã‚´URLã‚’å‰Šé™¤
            if any(keyword in col_lower for keyword in ['ãƒ–ãƒ¼ã‚¹ç•ªå·', 'å°é–“ç•ªå·', 'ãƒ­ã‚´url', 'ãƒ­ã‚´ url', 'logo', 'booth', 'å°é–“', 'ãƒ–ãƒ¼ã‚¹']):
                columns_to_drop.append(col)
                if debug_mode:
                    st.info(f"ğŸ—‘ï¸ ä¸è¦åˆ—å‰Šé™¤: {col}")
        
        if columns_to_drop:
            df = df.drop(columns=columns_to_drop)
        
        # åˆ—åæ­£è¦åŒ–ï¼ˆç°¡æ½”ç‰ˆï¼‰
        rename_dict = {}
        for col in df.columns:
            col_clean = str(col).strip()
            col_lower = col_clean.lower()
            
            # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            for old_name, new_name in COLUMN_RENAMES.items():
                if col_lower == old_name.lower():
                    rename_dict[col] = new_name
                    break
            
            # éƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæœ€å°é™ï¼‰
            if col not in rename_dict:
                if 'tel' in col_lower and 'url' not in col_lower:
                    rename_dict[col] = 'Tel'
                elif 'mail' in col_lower and 'url' not in col_lower:
                    rename_dict[col] = 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'
                elif any(kw in col_clean for kw in ['ä¼šç¤¾', 'ä¼æ¥­']) and 'url' not in col_lower:
                    rename_dict[col] = 'ä¼šç¤¾å'
                elif col_lower in ['url', 'website'] and len(col_clean) < 15:
                    rename_dict[col] = 'Website'
                elif any(kw in col_clean for kw in ['å±•ç¤ºä¼šåˆæ—¥', 'é–‹å§‹æ—¥', 'åˆæ—¥', 'é–‹å‚¬æ—¥']) and 'url' not in col_lower:
                    rename_dict[col] = 'å±•ç¤ºä¼šåˆæ—¥'
        
        # å®‰å…¨ãªåˆ—åå¤‰æ›
        try:
            df.rename(columns=rename_dict, inplace=True)
            # å†åº¦åˆ—åã‚’æ–‡å­—åˆ—ã«çµ±ä¸€
            df.columns = [str(col) for col in df.columns]
        except Exception as e:
            if debug_mode:
                st.warning(f"âš ï¸ åˆ—åå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å¿…é ˆåˆ—è¿½åŠ 
        for col in REQUIRED_COLUMNS:
            if col not in df.columns:
                df[col] = ""
        
        # å±•ç¤ºä¼šåˆæ—¥åˆ—ã‚‚è¿½åŠ 
        if 'å±•ç¤ºä¼šåˆæ—¥' not in df.columns:
            df['å±•ç¤ºä¼šåˆæ—¥'] = ""
        
        # å±•ç¤ºä¼šåè¨­å®š
        if 'å±•ç¤ºä¼šå' in df.columns:
            empty_mask = df['å±•ç¤ºä¼šå'].isna() | (df['å±•ç¤ºä¼šå'] == '')
            df.loc[empty_mask, 'å±•ç¤ºä¼šå'] = inferred_event_name
        
        # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ï¼ˆæœ€å°é™ï¼‰
        try:
            if "Tel" in df.columns:
                df["Tel"] = df["Tel"].apply(normalize_phone)
            if "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹" in df.columns:
                df["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"] = df["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"].apply(validate_email)
        except Exception as e:
            if debug_mode:
                st.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ‹…å½“è€…è£œå®Œï¼ˆä¿®æ­£ï¼šã€Œã”æ‹…å½“è€…ã€ã«å¤‰æ›´ï¼‰
        if "æ‹…å½“è€…" in df.columns:
            df["æ‹…å½“è€…"] = df["æ‹…å½“è€…"].fillna("ã”æ‹…å½“è€…").replace("", "ã”æ‹…å½“è€…")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        df['ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«'] = filename
        df['æ›´æ–°æ—¥æ™‚'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        df['å‡¦ç†æœˆ'] = datetime.now().strftime('%Y-%m')
        
        # æœ€çµ‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
        df = df.reset_index(drop=True)
        
        return df, stats, None
        
    except Exception as e:
        return None, {}, str(e)

def align_dataframe_columns(dfs, debug_mode=False):
    """DataFrameã®åˆ—åã‚’çµ±ä¸€ã—ã¦å®‰å…¨ã«çµåˆæº–å‚™"""
    if not dfs:
        return []
    
    # å…¨DataFrameã®åˆ—åã‚’åé›†
    all_columns = set()
    for df in dfs:
        all_columns.update(df.columns)
    
    all_columns = sorted(list(all_columns))
    
    if debug_mode:
        st.info(f"ğŸ”§ åˆ—åçµ±ä¸€: {len(all_columns)}å€‹ã®åˆ—ã‚’çµ±ä¸€")
    
    # å„DataFrameã‚’åŒã˜åˆ—æ§‹æˆã«çµ±ä¸€
    aligned_dfs = []
    for i, df in enumerate(dfs):
        try:
            # æ¬ ã‘ã¦ã„ã‚‹åˆ—ã‚’è¿½åŠ 
            for col in all_columns:
                if col not in df.columns:
                    df[col] = ""
            
            # åˆ—é †åºã‚’çµ±ä¸€
            df = df[all_columns]
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
            df = df.reset_index(drop=True)
            
            aligned_dfs.append(df)
            
        except Exception as e:
            if debug_mode:
                st.warning(f"âš ï¸ DataFrame{i}ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    return aligned_dfs

def concatenate_dataframes_safely(dfs, debug_mode=False):
    """å®‰å…¨ãªDataFrameçµåˆï¼ˆä»£æ›¿æ–¹æ³•ï¼‰"""
    if not dfs:
        return pd.DataFrame()
    
    if len(dfs) == 1:
        return dfs[0].reset_index(drop=True)
    
    # æœ€åˆã®DataFrameã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
    result_df = dfs[0].copy().reset_index(drop=True)
    base_columns = list(result_df.columns)
    
    for i, df in enumerate(dfs[1:], 1):
        try:
            df = df.reset_index(drop=True)
            df_columns = list(df.columns)
            
            # å…±é€šåˆ—ã®ã¿ã‚’ä¿æŒ
            common_columns = [col for col in base_columns if col in df_columns]
            
            # æ–°ã—ã„åˆ—ã‚’è¿½åŠ 
            new_columns = [col for col in df_columns if col not in base_columns]
            
            if new_columns and debug_mode:
                st.info(f"DataFrame{i}: æ–°è¦åˆ—è¿½åŠ  {new_columns}")
            
            # ãƒ™ãƒ¼ã‚¹DataFrameã«æ–°ã—ã„åˆ—ã‚’è¿½åŠ 
            for col in new_columns:
                result_df[col] = ""
                base_columns.append(col)
            
            # çµåˆã™ã‚‹DataFrameã«æ¬ ã‘ã¦ã„ã‚‹åˆ—ã‚’è¿½åŠ 
            for col in base_columns:
                if col not in df.columns:
                    df[col] = ""
            
            # åˆ—é †åºã‚’çµ±ä¸€
            df = df[base_columns]
            
            # è¡Œã‚’è¿½åŠ 
            result_df = pd.concat([result_df, df], ignore_index=True, sort=False)
            result_df = result_df.reset_index(drop=True)
            
        except Exception as e:
            if debug_mode:
                st.warning(f"âš ï¸ DataFrame{i}ã®çµåˆã‚¨ãƒ©ãƒ¼: {e} - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            continue
    
    return result_df

def remove_duplicates_lightweight(df, is_large=False):
    """è»½é‡åŒ–ã•ã‚ŒãŸé‡è¤‡å‰Šé™¤ï¼ˆæ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹è€ƒæ…®ï¼‰"""
    if is_large:
        st.info("ğŸ”„ é‡è¤‡å‰Šé™¤ä¸­...")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
    df = df.reset_index(drop=True)
    
    original_count = len(df)
    
    try:
        # ä¼šç¤¾å+å±•ç¤ºä¼šå+ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã§ã®é‡è¤‡å‰Šé™¤ï¼ˆæ—¥ä»˜ãƒ™ãƒ¼ã‚¹ï¼‰
        if 'ä¼šç¤¾å' in df.columns and 'å±•ç¤ºä¼šå' in df.columns:
            # æ—¥ä»˜åˆ—ã‚’ datetime å‹ã«å¤‰æ›
            date_cols_parsed = []
            
            # å±•ç¤ºä¼šåˆæ—¥ã®å¤‰æ›ã‚’è©¦ã¿ã‚‹
            if 'å±•ç¤ºä¼šåˆæ—¥' in df.columns:
                try:
                    df['_parsed_exhibition_date'] = pd.to_datetime(df['å±•ç¤ºä¼šåˆæ—¥'], errors='coerce')
                    date_cols_parsed.append('_parsed_exhibition_date')
                except:
                    pass
            
            # æœ€çµ‚æ›´æ–°æ—¥ã®å¤‰æ›ã‚’è©¦ã¿ã‚‹ï¼ˆã‚‚ã—å­˜åœ¨ã™ã‚Œã°ï¼‰
            if 'æœ€çµ‚æ›´æ–°æ—¥' in df.columns:
                try:
                    df['_parsed_update_date'] = pd.to_datetime(df['æœ€çµ‚æ›´æ–°æ—¥'], errors='coerce')
                    date_cols_parsed.append('_parsed_update_date')
                except:
                    pass
            
            # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
            if date_cols_parsed:
                # è¤‡æ•°ã®æ—¥ä»˜åˆ—ãŒã‚ã‚‹å ´åˆã¯ã€æœ€ã‚‚æ–°ã—ã„æ—¥ä»˜ã‚’ä½¿ç”¨
                if len(date_cols_parsed) > 1:
                    df['_max_date'] = df[date_cols_parsed].max(axis=1)
                    sort_col = '_max_date'
                else:
                    sort_col = date_cols_parsed[0]
                
                # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ã€NaTã¯æœ€å¾Œã«ï¼‰
                df = df.sort_values(by=sort_col, ascending=False, na_position='last')
            
            # é‡è¤‡å‰Šé™¤ã®åŸºæº–ã‚’è¨­å®š
            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã‚Œã‚‚é‡è¤‡åˆ¤å®šã«å«ã‚ã‚‹
            if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in df.columns:
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ­£è¦åŒ–ï¼ˆç©ºæ–‡å­—ã¨NaNã‚’çµ±ä¸€ï¼‰
                df['_normalized_email'] = df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].fillna('').str.strip()
                df.loc[df['_normalized_email'] == '', '_normalized_email'] = pd.NA
                
                # ä¼šç¤¾å + å±•ç¤ºä¼šå + ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã§é‡è¤‡å‰Šé™¤
                duplicate_subset = ['ä¼šç¤¾å', 'å±•ç¤ºä¼šå', '_normalized_email']
            else:
                duplicate_subset = ['ä¼šç¤¾å', 'å±•ç¤ºä¼šå']
            
            # é‡è¤‡å‰Šé™¤ï¼ˆæœ€åˆã®ã‚‚ã®=æœ€æ–°ã‚’æ®‹ã™ï¼‰
            df = df.drop_duplicates(subset=duplicate_subset, keep='first')
            
            # ä¸€æ™‚çš„ãªåˆ—ã‚’å‰Šé™¤
            temp_cols = ['_parsed_exhibition_date', '_parsed_update_date', '_max_date', '_normalized_email']
            for col in temp_cols:
                if col in df.columns:
                    df = df.drop(columns=[col])
            
            df = df.reset_index(drop=True)
        
        removed_count = original_count - len(df)
        if is_large and removed_count > 0:
            st.info(f"ğŸ—‘ï¸ {removed_count}ä»¶ã®é‡è¤‡è¡Œã‚’å‰Šé™¤ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒç•°ãªã‚‹å ´åˆã¯åˆ¥ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿æŒï¼‰")
    
    except Exception as e:
        st.warning(f"âš ï¸ é‡è¤‡å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    
    return df

def display_processed_files():
    """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º"""
    if st.session_state.processed_files:
        st.subheader("ğŸ“‹ å‡¦ç†çµæœè©³ç´°")
        df_status = pd.DataFrame(st.session_state.processed_files)
        st.dataframe(df_status, use_container_width=True)
        
        # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
        if st.session_state.processing_stats:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å‡¦ç†æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«", st.session_state.processing_stats.get('files_processed', 0))
            with col2:
                st.metric("ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºæ•°", st.session_state.processing_stats.get('email_extracted', 0))
            with col3:
                st.metric("é›»è©±ç•ªå·æŠ½å‡ºæ•°", st.session_state.processing_stats.get('tel_extracted', 0))

def data_search_and_download():
    """ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½"""
    if st.session_state.merged_data.empty:
        st.info("âš ï¸ ã¾ãšãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦çµ±åˆã—ã¦ãã ã•ã„")
        return
        
    st.subheader("ğŸ” é«˜åº¦ãªæ¤œç´¢ãƒ»åˆ†æãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(st.session_state.merged_data))
    with col2:
        unique_companies = st.session_state.merged_data['ä¼šç¤¾å'].nunique()
        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°", unique_companies)
    with col3:
        unique_exhibitions = st.session_state.merged_data['å±•ç¤ºä¼šå'].nunique()
        st.metric("å±•ç¤ºä¼šæ•°", unique_exhibitions)
    with col4:
        email_with_data = len(st.session_state.merged_data[
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & 
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
        ])
        st.metric("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚ã‚Š", email_with_data)
    
    # è©³ç´°çµ±è¨ˆ
    with st.expander("ğŸ“Š è©³ç´°çµ±è¨ˆæƒ…å ±"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**å±•ç¤ºä¼šåˆ¥ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆä¸Šä½10ä½ï¼‰**")
            exhibition_counts = st.session_state.merged_data['å±•ç¤ºä¼šå'].value_counts().head(10)
            for idx, (exhibition, count) in enumerate(exhibition_counts.items(), 1):
                st.write(f"{idx}. {exhibition}: {count}ä»¶")
        
        with col2:
            st.write("**æ¥­ç•Œåˆ¥ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆä¸Šä½10ä½ï¼‰**")
            industry_counts = st.session_state.merged_data['æ¥­ç•Œ'].value_counts().head(10)
            for idx, (industry, count) in enumerate(industry_counts.items(), 1):
                st.write(f"{idx}. {industry}: {count}ä»¶")
    
    # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.markdown("### ğŸ¯ è©³ç´°æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    col1, col2 = st.columns(2)
    
    with col1:
        # å±•ç¤ºä¼šåãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        exhibitions = ['å…¨ã¦'] + sorted(st.session_state.merged_data['å±•ç¤ºä¼šå'].dropna().unique().tolist())
        selected_exhibitions = st.multiselect("å±•ç¤ºä¼šåï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", exhibitions, default=['å…¨ã¦'])
        
        # æ¥­ç•Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        industries = ['å…¨ã¦'] + sorted(st.session_state.merged_data['æ¥­ç•Œ'].dropna().unique().tolist())
        selected_industries = st.multiselect("æ¥­ç•Œï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", industries, default=['å…¨ã¦'])
        
        # ä¼šç¤¾åæ¤œç´¢
        company_search = st.text_input("ä¼šç¤¾åæ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")
    
    with col2:
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æœ‰ç„¡
        email_filter = st.selectbox("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", ['å…¨ã¦', 'ã‚ã‚Š', 'ãªã—'])
        
        # é›»è©±ç•ªå·æœ‰ç„¡
        tel_filter = st.selectbox("é›»è©±ç•ªå·", ['å…¨ã¦', 'ã‚ã‚Š', 'ãªã—'])
        
        # æ‹…å½“è€…æ¤œç´¢
        contact_search = st.text_input("æ‹…å½“è€…åæ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_data = st.session_state.merged_data.copy()
    
    # è¤‡æ•°é¸æŠå¯¾å¿œ
    if 'å…¨ã¦' not in selected_exhibitions:
        filtered_data = filtered_data[filtered_data['å±•ç¤ºä¼šå'].isin(selected_exhibitions)]
    
    if 'å…¨ã¦' not in selected_industries:
        filtered_data = filtered_data[filtered_data['æ¥­ç•Œ'].isin(selected_industries)]
    
    if company_search:
        filtered_data = filtered_data[
            filtered_data['ä¼šç¤¾å'].str.contains(company_search, na=False, case=False)
        ]
    
    if contact_search:
        filtered_data = filtered_data[
            filtered_data['æ‹…å½“è€…'].str.contains(contact_search, na=False, case=False)
        ]
    
    if email_filter == 'ã‚ã‚Š':
        filtered_data = filtered_data[
            filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna() & 
            (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
        ]
    elif email_filter == 'ãªã—':
        filtered_data = filtered_data[
            filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].isna() | 
            (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] == '')
        ]
    
    if tel_filter == 'ã‚ã‚Š':
        filtered_data = filtered_data[
            filtered_data['Tel'].notna() & 
            (filtered_data['Tel'] != '')
        ]
    elif tel_filter == 'ãªã—':
        filtered_data = filtered_data[
            filtered_data['Tel'].isna() | 
            (filtered_data['Tel'] == '')
        ]
    
    # æ¤œç´¢çµæœè¡¨ç¤º
    st.markdown(f"### ğŸ¯ æ¤œç´¢çµæœ: **{len(filtered_data)}ä»¶**")
    
    if not filtered_data.empty:
        # çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¨­å®š
        preview_limit = st.slider("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºä»¶æ•°", 10, 1000, 100)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        st.dataframe(filtered_data.head(preview_limit), use_container_width=True)
        
        if len(filtered_data) > preview_limit:
            st.info(f"ğŸ’¡ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯æœ€åˆã®{preview_limit}ä»¶ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚å…¨{len(filtered_data)}ä»¶")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.markdown("### ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        with st.expander("âš™ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=True):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                download_email_only = st.checkbox(
                    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚ã‚Šã®ã¿",
                    help="ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
                )
            
            with col2:
                # æ¥­ç•Œã‚«ãƒ†ã‚´ãƒªé¸æŠ
                if 'æ¥­ç•Œ' in filtered_data.columns:
                    unique_industries = filtered_data['æ¥­ç•Œ'].dropna().unique().tolist()
                    selected_industries = st.multiselect(
                        "æ¥­ç•Œã‚’é¸æŠ",
                        options=unique_industries,
                        default=unique_industries,
                        help="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ¥­ç•Œã‚’é¸æŠ"
                    )
            
            with col3:
                # å±•ç¤ºä¼šé¸æŠ
                if 'å±•ç¤ºä¼šå' in filtered_data.columns:
                    unique_exhibitions = filtered_data['å±•ç¤ºä¼šå'].dropna().unique().tolist()
                    selected_exhibitions = st.multiselect(
                        "å±•ç¤ºä¼šã‚’é¸æŠ",
                        options=unique_exhibitions,
                        default=unique_exhibitions,
                        help="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å±•ç¤ºä¼šã‚’é¸æŠ"
                    )
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿
        download_data = filtered_data.copy()
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚ã‚Šã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        if download_email_only:
            download_data = download_data[
                (download_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & 
                (download_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
            ]
        
        # æ¥­ç•Œãƒ•ã‚£ãƒ«ã‚¿
        if 'æ¥­ç•Œ' in download_data.columns and 'selected_industries' in locals():
            download_data = download_data[download_data['æ¥­ç•Œ'].isin(selected_industries)]
        
        # å±•ç¤ºä¼šãƒ•ã‚£ãƒ«ã‚¿
        if 'å±•ç¤ºä¼šå' in download_data.columns and 'selected_exhibitions' in locals():
            download_data = download_data[download_data['å±•ç¤ºä¼šå'].isin(selected_exhibitions)]
        
        st.info(f"ğŸ¯ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡: {len(download_data)}ä»¶")
        
        # SNSåˆ—ã®çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œ
        download_data = merge_sns_columns(download_data)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            csv = download_data.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"exhibition_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
        with col2:
            # Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            try:
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    download_data.to_excel(writer, index=False, sheet_name='ExhibitionData')
                excel_data = output.getvalue()
                
                st.download_button(
                    label="ğŸ“Š Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=excel_data,
                    file_name=f"exhibition_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            except ImportError:
                st.error("âš ï¸ Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¯openpyxlãŒå¿…è¦ã§ã™ã€‚\n`pip install openpyxl`ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        with col3:
            # ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã®ã¿ï¼ˆä¿®æ­£ï¼šå±•ç¤ºä¼šåˆæ—¥ã‚’è¿½åŠ ï¼‰
            email_columns = ['ä¼šç¤¾å', 'æ‹…å½“è€…', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'å±•ç¤ºä¼šå', 'æ¥­ç•Œ']
            if 'å±•ç¤ºä¼šåˆæ—¥' in filtered_data.columns:
                email_columns.append('å±•ç¤ºä¼šåˆæ—¥')
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            email_source = download_data if not download_data.empty else filtered_data
            email_only = email_source[
                (email_source['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & 
                (email_source['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
            ]
            # å¿…è¦ãªåˆ—ã®ã¿é¸æŠï¼ˆå­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ï¼‰
            available_email_cols = [col for col in email_columns if col in email_only.columns]
            email_only = email_only[available_email_cols] if available_email_cols else email_only
            
            email_csv = email_only.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“§ ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ",
                data=email_csv,
                file_name=f"email_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
        with col4:
            # ãƒ†ãƒ¬ã‚¢ãƒãƒªã‚¹ãƒˆï¼ˆä¿®æ­£ï¼šå±•ç¤ºä¼šåˆæ—¥ã‚’è¿½åŠ ï¼‰
            tel_columns = ['ä¼šç¤¾å', 'æ‹…å½“è€…', 'Tel', 'å±•ç¤ºä¼šå', 'æ¥­ç•Œ']
            if 'å±•ç¤ºä¼šåˆæ—¥' in filtered_data.columns:
                tel_columns.append('å±•ç¤ºä¼šåˆæ—¥')
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            tel_source = download_data if not download_data.empty else filtered_data
            tel_only = tel_source[
                (tel_source['Tel'].notna()) & 
                (tel_source['Tel'] != '')
            ]
            # å¿…è¦ãªåˆ—ã®ã¿é¸æŠï¼ˆå­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ï¼‰
            available_tel_cols = [col for col in tel_columns if col in tel_only.columns]
            tel_only = tel_only[available_tel_cols] if available_tel_cols else tel_only
            
            tel_csv = tel_only.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“ ãƒ†ãƒ¬ã‚¢ãƒãƒªã‚¹ãƒˆ",
                data=tel_csv,
                file_name=f"tel_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
        # è©³ç´°çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
        with st.expander("ğŸ“ˆ æ¤œç´¢çµæœã®è©³ç´°åˆ†æ"):
            col1, col2 = st.columns(2)
            
            with col1:
                stats_data = {
                    'é …ç›®': [
                        'ç·ãƒ‡ãƒ¼ã‚¿æ•°', 'ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°', 'å±•ç¤ºä¼šæ•°', 'æ¥­ç•Œæ•°', 
                        'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚ã‚Š', 'é›»è©±ç•ªå·ã‚ã‚Š', 'ä¸¡æ–¹ã‚ã‚Š'
                    ],
                    'ä»¶æ•°': [
                        len(filtered_data),
                        filtered_data['ä¼šç¤¾å'].nunique(),
                        filtered_data['å±•ç¤ºä¼šå'].nunique(),
                        filtered_data['æ¥­ç•Œ'].nunique(),
                        len(filtered_data[(filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')]),
                        len(filtered_data[(filtered_data['Tel'].notna()) & (filtered_data['Tel'] != '')]),
                        len(filtered_data[
                            (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '') &
                            (filtered_data['Tel'].notna()) & (filtered_data['Tel'] != '')
                        ])
                    ]
                }
                stats_df = pd.DataFrame(stats_data)
                st.dataframe(stats_df, use_container_width=True)
            
            with col2:
                # æ›´æ–°æ—¥æ™‚åˆ¥é›†è¨ˆ
                if 'æ›´æ–°æ—¥æ™‚' in filtered_data.columns:
                    st.write("**æ›´æ–°æ—¥æ™‚åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°**")
                    update_counts = filtered_data['æ›´æ–°æ—¥æ™‚'].value_counts().head(10)
                    for date, count in update_counts.items():
                        st.write(f"- {date}: {count}ä»¶")
    else:
        st.warning("ğŸ” æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

def main():
    st.title("ğŸ“Š å±•ç¤ºä¼šãƒªã‚¹ãƒˆè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé«˜æ©Ÿèƒ½ç‰ˆï¼‰")
    st.markdown("""
    **ğŸš€ æ©Ÿèƒ½ä¸€è¦§:**
    - Notionã‹ã‚‰ã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGoogleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå¯¾å¿œï¼‰
    - é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆé€£çµ¡å…ˆã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ãƒ»é›»è©±ç•ªå·è‡ªå‹•æŠ½å‡ºï¼‰
    - åˆ—åã®è‡ªå‹•æ­£è¦åŒ–ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    - é‡è¤‡å‰Šé™¤ãƒ»ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    - é«˜åº¦ãªæ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆãƒ»ãƒ†ãƒ¬ã‚¢ãƒãƒªã‚¹ãƒˆç”Ÿæˆ
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    st.sidebar.title("ğŸ“Œ æ©Ÿèƒ½é¸æŠ")
    mode = st.sidebar.radio(
        "å‡¦ç†ã‚’é¸æŠã—ã¦ãã ã•ã„:",
        ["ğŸ”— Notioné€£æº", "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†", "ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»åˆ†æ"]
    )
    
    if mode == "ğŸ”— Notioné€£æº":
        notion_download()
    elif mode == "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†":
        file_upload_processing()
        display_processed_files()
    elif mode == "ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»åˆ†æ":
        data_search_and_download()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®çµ±è¨ˆæƒ…å ±
    if not st.session_state.merged_data.empty:
        st.sidebar.markdown("---")
        st.sidebar.markdown("ğŸ“Š **ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³**")
        st.sidebar.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(st.session_state.merged_data))
        st.sidebar.metric("ä¼æ¥­æ•°", st.session_state.merged_data['ä¼šç¤¾å'].nunique())
        email_count = len(st.session_state.merged_data[
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & 
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
        ])
        st.sidebar.metric("ãƒ¡ãƒ¼ãƒ«ã‚ã‚Š", email_count)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ
    if st.sidebar.button("ğŸ”„ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state.merged_data = pd.DataFrame()
        st.session_state.processed_files = []
        st.session_state.processing_stats = {}
        if 'notion_files' in st.session_state:
            del st.session_state.notion_files
        st.sidebar.success("âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
        st.rerun()

if __name__ == "__main__":
    main()