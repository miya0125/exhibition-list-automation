import streamlit as st
import pandas as pd
import os
import requests
from datetime import datetime
import re
import tempfile
import zipfile
import io
import logging
from charset_normalizer import detect
from notion_client import Client
import json

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å±•ç¤ºä¼šãƒªã‚¹ãƒˆè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé«˜æ©Ÿèƒ½ç‰ˆï¼‰",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'merged_data' not in st.session_state:
    st.session_state.merged_data = pd.DataFrame()
if 'processed_files' not in st.session_state:
    st.session_state.processed_files = []
if 'processing_stats' not in st.session_state:
    st.session_state.processing_stats = {}

# æ‹¡å¼µç‰ˆåˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
COLUMN_RENAMES = {
    # ä¼šç¤¾åé–¢é€£
    'ä¼šç¤¾å': 'ä¼šç¤¾å', 'ä¼æ¥­å': 'ä¼šç¤¾å', 'å‡ºå±•ç¤¾å': 'ä¼šç¤¾å',
    'å‡ºå±•ç¤¾': 'ä¼šç¤¾å', 'æ³•äººå': 'ä¼šç¤¾å', 'COMPANY': 'ä¼šç¤¾å', 
    'ç¤¾å': 'ä¼šç¤¾å', 'Company': 'ä¼šç¤¾å', 'company': 'ä¼šç¤¾å',
    'å‡ºå±•ä¼æ¥­å': 'ä¼šç¤¾å', 'å‡ºå±•è€…å': 'ä¼šç¤¾å', 'Corporate Name': 'ä¼šç¤¾å',
    'çµ„ç¹”å': 'ä¼šç¤¾å', 'Organization': 'ä¼šç¤¾å', 'å›£ä½“å': 'ä¼šç¤¾å',
    
    # æ‹…å½“è€…é–¢é€£
    'æ‹…å½“è€…å': 'æ‹…å½“è€…', 'æ°å': 'æ‹…å½“è€…', 'åå‰': 'æ‹…å½“è€…', 
    'æ‹…å½“': 'æ‹…å½“è€…', 'æ‹…å½“çª“å£': 'æ‹…å½“è€…', 'è²¬ä»»è€…': 'æ‹…å½“è€…', 
    'ã”æ‹…å½“è€…': 'æ‹…å½“è€…', 'Contact Person': 'æ‹…å½“è€…', 'Person': 'æ‹…å½“è€…',
    'å–¶æ¥­æ‹…å½“': 'æ‹…å½“è€…', 'ä»£è¡¨è€…': 'æ‹…å½“è€…', 'æ‹…å½“è€…æ§˜': 'æ‹…å½“è€…',
    'ãŠåå‰': 'æ‹…å½“è€…', 'Name': 'æ‹…å½“è€…', 'é€£çµ¡æ‹…å½“è€…': 'æ‹…å½“è€…',
    
    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹é–¢é€£
    'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'ãƒ¡ãƒ¼ãƒ«': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'e-mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'Eãƒ¡ãƒ¼ãƒ«': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'Email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 
    'E-Mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'email': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'Mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'mail': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'E-mailã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'Emailã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    'ãƒ¡ã‚¢ãƒ‰': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'Mail Address': 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
    
    # æ¥­ç•Œé–¢é€£
    'æ¥­ç•Œå': 'æ¥­ç•Œ', 'æ¥­ç•Œ': 'æ¥­ç•Œ', 'æ¥­ç¨®': 'æ¥­ç•Œ', 
    'åˆ†é‡': 'æ¥­ç•Œ', 'Industry': 'æ¥­ç•Œ', 'äº‹æ¥­åˆ†é‡': 'æ¥­ç•Œ',
    'ã‚«ãƒ†ã‚´ãƒªãƒ¼': 'æ¥­ç•Œ', 'Category': 'æ¥­ç•Œ', 'éƒ¨é–€': 'æ¥­ç•Œ',
    'æ¥­æ…‹': 'æ¥­ç•Œ', 'Sector': 'æ¥­ç•Œ', 'ç”£æ¥­': 'æ¥­ç•Œ',
    
    # å±•ç¤ºä¼šåé–¢é€£
    'å±•ç¤ºä¼šå': 'å±•ç¤ºä¼šå', 'å±•è¦§ä¼š': 'å±•ç¤ºä¼šå', 'å±•ç¤ºä¼š': 'å±•ç¤ºä¼šå', 
    'EXPO': 'å±•ç¤ºä¼šå', 'ã‚¤ãƒ™ãƒ³ãƒˆå': 'å±•ç¤ºä¼šå', 'Event': 'å±•ç¤ºä¼šå',
    'Exhibition': 'å±•ç¤ºä¼šå', 'Show': 'å±•ç¤ºä¼šå', 'Fair': 'å±•ç¤ºä¼šå',
    'è¦‹æœ¬å¸‚': 'å±•ç¤ºä¼šå', 'Trade Show': 'å±•ç¤ºä¼šå', 'ã‚¤ãƒ™ãƒ³ãƒˆ': 'å±•ç¤ºä¼šå',
    
    # é›»è©±ç•ªå·é–¢é€£
    'é›»è©±ç•ªå·': 'Tel', 'Tel': 'Tel', 'é›»è©±': 'Tel', 'TEL': 'Tel', 'Phone': 'Tel', 
    'TEL_FAX': 'Tel', 'TELã¨FAX': 'Tel', 'tel': 'Tel', 'phone': 'Tel',
    'Telephone': 'Tel', 'Phone Number': 'Tel', 'é€£çµ¡å…ˆé›»è©±': 'Tel',
    'TELç•ªå·': 'Tel', 'Telç•ªå·': 'Tel', 'â„¡': 'Tel',
    
    # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆé–¢é€£
    'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ': 'Website', 'WEBã‚µã‚¤ãƒˆ': 'Website', 'HP': 'Website', 
    'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸': 'Website', 'URL': 'Website', 'Web': 'Website', 'web': 'Website',
    'Website URL': 'Website', 'ã‚¦ã‚§ãƒ–': 'Website', 'WEB': 'Website', 'ã‚µã‚¤ãƒˆ': 'Website',
    'Homepage': 'Website', 'Webã‚µã‚¤ãƒˆURL': 'Website', 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURL': 'Website',
    
    # éƒµä¾¿ç•ªå·é–¢é€£
    'éƒµä¾¿ç•ªå·': 'YuubinBangou', 'ã€’': 'YuubinBangou', 'ZIPã‚³ãƒ¼ãƒ‰': 'YuubinBangou', 
    'ZIP': 'YuubinBangou', 'Zip Code': 'YuubinBangou', 'Postal Code': 'YuubinBangou',
    'éƒµä¾¿': 'YuubinBangou', 'ã‚†ã†ã³ã‚“ç•ªå·': 'YuubinBangou',
    
    # ä½æ‰€é–¢é€£
    'ä½æ‰€': 'Address', 'æ‰€åœ¨åœ°': 'Address', 'ã‚¢ãƒ‰ãƒ¬ã‚¹': 'Address', 'æœ¬ç¤¾': 'Address', 
    'æœ¬ç¤¾æ‰€åœ¨åœ°': 'Address', 'æœ¬ç¤¾ä½æ‰€': 'Address', 'ä¼šç¤¾ä½æ‰€': 'Address',
    'Location': 'Address', 'äº‹æ¥­æ‰€': 'Address', 'å–¶æ¥­æ‰€': 'Address',
    'æ‰€åœ¨': 'Address', 'ç•ªåœ°': 'Address',
    
    # FAXé–¢é€£
    'FAXç•ªå·': 'FAX', 'FAX': 'FAX', 'Fax': 'FAX', 'ãƒ•ã‚¡ãƒƒã‚¯ã‚¹': 'FAX',
    'fax': 'FAX', 'ãƒ•ã‚¡ã‚¯ã‚¹': 'FAX', 'Facsimile': 'FAX',
    
    # å•ã„åˆã‚ã›å…ˆé–¢é€£
    'å•ã„åˆã‚ã›å…ˆ': 'å•ã„åˆã‚ã›å…ˆ', 'é€£çµ¡å…ˆ': 'é€£çµ¡å…ˆ', 'ãŠå•ã„åˆã‚ã›å…ˆ': 'ãŠå•ã„åˆã‚ã›å…ˆ', 
    'Contact': 'é€£çµ¡å…ˆ', 'ãŠå•åˆã›å…ˆ': 'ãŠå•ã„åˆã‚ã›å…ˆ', 'å•åˆã›å…ˆ': 'å•ã„åˆã‚ã›å…ˆ',
    'Contact Information': 'é€£çµ¡å…ˆ', 'é€£çµ¡å…ˆæƒ…å ±': 'é€£çµ¡å…ˆ',
    
    # ãã®ä»–
    'å‚™è€ƒ': 'å‚™è€ƒ', 'ãƒ¡ãƒ¢': 'å‚™è€ƒ', 'Note': 'å‚™è€ƒ', 'Memo': 'å‚™è€ƒ', 'ã‚³ãƒ¡ãƒ³ãƒˆ': 'å‚™è€ƒ',
    'éƒ¨ç½²': 'éƒ¨ç½²', 'Department': 'éƒ¨ç½²', 'æ‰€å±': 'éƒ¨ç½²', 'Division': 'éƒ¨ç½²',
    'å½¹è·': 'å½¹è·', 'Title': 'å½¹è·', 'Position': 'å½¹è·', 'è‚©æ›¸': 'å½¹è·'
}

REQUIRED_COLUMNS = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "å±•ç¤ºä¼šå", "æ‹…å½“è€…", "æ¥­ç•Œ", "Tel", "ä¼šç¤¾å"]
KEY_COLS = ["å±•ç¤ºä¼šå", "æ¥­ç•Œ", "ä¼šç¤¾å"]

# ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»æ­£è¦åŒ–é–¢æ•°
def extract_email_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º"""
    if pd.isna(text) or text == "":
        return ""
    
    text = str(text)
    patterns = [
        r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
        r'([a-zA-Z0-9._%+-]+[@ï¼ ][a-zA-Z0-9.-]+[.ï¼][a-zA-Z]{2,})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            email = match.group(1)
            email = email.replace('ï¼ ', '@').replace('ï¼', '.')
            return email.lower()
    
    return ""

def extract_phone_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é›»è©±ç•ªå·ã‚’æŠ½å‡º"""
    if pd.isna(text) or text == "":
        return ""
    
    text = str(text)
    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼ˆï¼‰', '0123456789-()'))
    
    patterns = [
        r'(?:TEL|Tel|tel|é›»è©±|â„¡)[:ï¼š]?\s*([0-9\-\(\)\s]+)',
        r'(?:^|[\s])([0-9]{2,4}[\-][0-9]{2,4}[\-][0-9]{3,4})',
        r'(?:^|[\s])(\([0-9]{2,4}\)[0-9]{2,4}[\-]?[0-9]{3,4})',
        r'(?:^|[\s])([0-9]{10,11})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            phone = match.group(1).strip()
            if len(re.sub(r'[^0-9]', '', phone)) >= 10:
                return normalize_phone(phone)
    
    return ""

def normalize_phone(phone_str):
    """é›»è©±ç•ªå·ã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ•´ç†"""
    if pd.isna(phone_str) or phone_str == "":
        return phone_str
    
    phone_str = str(phone_str).strip()
    phone_str = phone_str.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ï¼ˆï¼‰', '0123456789-()'))
    phone_str = re.sub(r'[^\d\-+\(\)]', '', phone_str)
    
    digits_only = re.sub(r'[^\d]', '', phone_str)
    if len(digits_only) == 10 and digits_only[0] == '0':
        return f"{digits_only[:2]}-{digits_only[2:6]}-{digits_only[6:]}"
    elif len(digits_only) == 11 and digits_only[0] == '0':
        return f"{digits_only[:3]}-{digits_only[3:7]}-{digits_only[7:]}"
    
    return phone_str

def validate_email(email):
    """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
    if pd.isna(email) or email == "":
        return email
    
    email = str(email).strip().lower()
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    if re.match(pattern, email):
        return email
    return ""

def is_google_sheet_url(url):
    """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã©ã†ã‹åˆ¤å®š"""
    patterns = [
        r"https://docs\.google\.com/spreadsheets/d/([a-zA-Z0-9-_]+)",
        r"https://drive\.google\.com/file/d/([a-zA-Z0-9-_]+)",
        r"https://drive\.google\.com/open\?id=([a-zA-Z0-9-_]+)"
    ]
    for pattern in patterns:
        match = re.match(pattern, url)
        if match:
            return match
    return None

def extract_sheet_id(url):
    """URLã‹ã‚‰ã‚·ãƒ¼ãƒˆIDã‚’æŠ½å‡º"""
    match = re.search(r"/spreadsheets/d/([a-zA-Z0-9-_]+)", url)
    if match:
        return match.group(1)
    
    match = re.search(r"/file/d/([a-zA-Z0-9-_]+)", url)
    if match:
        return match.group(1)
    
    match = re.search(r"[?&]id=([a-zA-Z0-9-_]+)", url)
    if match:
        return match.group(1)
    
    return None

def google_sheet_to_csv_url(sheet_url):
    """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURLã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã«å¤‰æ›"""
    sheet_id = extract_sheet_id(sheet_url)
    if not sheet_id:
        return None
    
    gid_match = re.search(r"[#&]gid=([0-9]+)", sheet_url)
    gid = gid_match.group(1) if gid_match else "0"
    return f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}"

def process_dataframe(df, filename):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®é«˜åº¦ãªå‡¦ç†"""
    try:
        stats = {"email_extracted": 0, "tel_extracted": 0}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å±•ç¤ºä¼šåã‚’æ¨æ¸¬
        inferred_event_name = os.path.splitext(filename)[0]
        
        # åˆ—åæ­£è¦åŒ–ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ï¼‰
        rename_dict = {}
        for col in df.columns:
            col_lower = col.lower().strip()
            for old_name, new_name in COLUMN_RENAMES.items():
                if col_lower == old_name.lower():
                    rename_dict[col] = new_name
                    break
        
        df.rename(columns=rename_dict, inplace=True)
        
        # é€£çµ¡å…ˆç³»åˆ—ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨TELæŠ½å‡º
        contact_cols = [c for c in df.columns if any(keyword in c for keyword in 
                       ['å•ã„åˆã‚ã›å…ˆ', 'é€£çµ¡å…ˆ', 'ãŠå•ã„åˆã‚ã›å…ˆ', 'Contact', 'å•åˆã›å…ˆ'])]
        
        if contact_cols:
            # å¿…è¦ã‚«ãƒ©ãƒ ã‚’äº‹å‰ã«ç¢ºä¿
            if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' not in df.columns:
                df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] = ''
            if 'Tel' not in df.columns:
                df['Tel'] = ''

            for col in contact_cols:
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
                temp_emails = df[col].apply(extract_email_from_text)
                email_mask = (df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] == '') & (temp_emails != '')
                df.loc[email_mask, 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] = temp_emails[email_mask]
                stats["email_extracted"] += email_mask.sum()
                
                # é›»è©±ç•ªå·æŠ½å‡º
                temp_phones = df[col].apply(extract_phone_from_text)
                phone_mask = (df['Tel'] == '') & (temp_phones != '')
                df.loc[phone_mask, 'Tel'] = temp_phones[phone_mask]
                stats["tel_extracted"] += phone_mask.sum()
        
        # å¿…é ˆåˆ—ãŒãªã‘ã‚Œã°è¿½åŠ 
        for col in REQUIRED_COLUMNS:
            if col not in df.columns:
                df[col] = ""
        
        # å±•ç¤ºä¼šåãŒç©ºã®å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
        if 'å±•ç¤ºä¼šå' in df.columns:
            df.loc[df['å±•ç¤ºä¼šå'] == '', 'å±•ç¤ºä¼šå'] = inferred_event_name
        
        # æ–‡å­—åˆ—åˆ—ã®å‰å¾Œç©ºç™½é™¤å»
        str_cols = df.select_dtypes(include="object").columns
        df[str_cols] = df[str_cols].apply(lambda s: s.str.strip() if hasattr(s, 'str') else s)
        
        # æ‹…å½“è€…åã®è£œå®Œ
        if "æ‹…å½“è€…" in df.columns:
            df["æ‹…å½“è€…"] = df["æ‹…å½“è€…"].fillna("").replace("", "ã”æ‹…å½“è€…æ§˜")
        
        # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        if "Tel" in df.columns:
            df["Tel"] = df["Tel"].apply(normalize_phone)
        if "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹" in df.columns:
            df["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"] = df["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹"].apply(validate_email)
        
        # å¿…é ˆ3åˆ—ãŒä¸¸ã”ã¨ç©ºãªã‚‰ã‚¨ãƒ©ãƒ¼
        key_cols_check = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "Tel", "ä¼šç¤¾å"]
        if df[key_cols_check].replace("", pd.NA).isna().all().any():
            raise ValueError("å¿…é ˆé …ç›®ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ»Telãƒ»ä¼šç¤¾åï¼‰ã®åˆ—å…¨ä½“ãŒç©ºæ¬„ã§ã™ã€‚")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã¨æ›´æ–°æ—¥æ™‚ã‚’è¿½åŠ 
        df['ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«'] = filename
        df['æ›´æ–°æ—¥æ™‚'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        return df, stats, None
        
    except Exception as e:
        return None, {}, str(e)

def notion_download():
    """Notion APIã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    st.subheader("ğŸ”— Notion APIã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    # APIè¨­å®š
    col1, col2 = st.columns(2)
    with col1:
        notion_api_key = st.text_input("Notion API Key", type="password", 
                                     value=os.environ.get("NOTION_API_KEY", ""))
    with col2:
        database_id = st.text_input("Database ID", 
                                  value=os.environ.get("DATABASE_ID", ""))
    
    if st.button("Notionã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", disabled=not (notion_api_key and database_id)):
        try:
            notion = Client(auth=notion_api_key)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶
            filter_conditions = {
                "and": [
                    {
                        "or": [
                            {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ã‚ã‚Š"}},
                            {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "TELã¨URL"}},
                            {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åãƒ»ä½æ‰€ãƒ»URL"}},
                            {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "Telã€ä½æ‰€ã€URL"}},
                            {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åã¨URLï¼ˆç›´ã§ä¼æ¥­HPãƒªãƒ³ã‚¯ï¼‰"}},
                            {"property": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆæœ‰ç„¡ï¼‰", "select": {"equals": "ç¤¾åã¨URLã®ã¿"}}
                        ]
                    },
                    {"property": "ãƒ•ã‚¡ã‚¤ãƒ«", "files": {"is_not_empty": True}}
                ]
            }
            
            with st.spinner("Notionã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                # å…¨ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
                all_items = []
                start_cursor = None
                
                while True:
                    response = notion.databases.query(
                        database_id=database_id,
                        filter=filter_conditions,
                        start_cursor=start_cursor
                    )
                    items = response.get("results", [])
                    all_items.extend(items)
                    if not response.get("has_more"):
                        break
                    start_cursor = response.get("next_cursor")
                
                st.success(f"{len(all_items)}ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†
                downloaded_files = []
                failed_files = []
                progress_bar = st.progress(0)
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                for item_idx, item in enumerate(all_items):
                    properties = item["properties"]
                    
                    # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    page_title = "untitled"
                    if "åå‰" in properties and properties["åå‰"]["title"]:
                        page_title = properties["åå‰"]["title"][0]["plain_text"] if properties["åå‰"]["title"] else "untitled"
                    elif "Name" in properties and properties["Name"]["title"]:
                        page_title = properties["Name"]["title"][0]["plain_text"] if properties["Name"]["title"] else "untitled"
                    
                    file_property = properties.get("ãƒ•ã‚¡ã‚¤ãƒ«")
                    if file_property and file_property["type"] == "files":
                        for file_idx, file_info in enumerate(file_property["files"]):
                            try:
                                # Notionã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
                                if file_info["type"] == "file":
                                    file_name = file_info["name"]
                                    file_url = file_info["file"]["url"]
                                    ext = os.path.splitext(file_name)[1].lower()
                                    
                                    if ext in [".csv", ".xlsx", ".xls"]:
                                        response = requests.get(file_url, headers=headers)
                                        response.raise_for_status()
                                        final_name = f"{os.path.splitext(file_name)[0]}_{item_idx+1}_{file_idx+1}{ext}"
                                        downloaded_files.append((final_name, response.content))
                                
                                # Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç­‰ã®å¤–éƒ¨URL
                                elif file_info["type"] == "external":
                                    url = file_info["external"]["url"]
                                    if is_google_sheet_url(url):
                                        csv_url = google_sheet_to_csv_url(url)
                                        if csv_url:
                                            response = requests.get(csv_url, headers=headers)
                                            response.raise_for_status()
                                            sheet_id = extract_sheet_id(url)
                                            file_name = f"{page_title}_{sheet_id}_{item_idx+1}_{file_idx+1}.csv"
                                            downloaded_files.append((file_name, response.content))
                                        
                            except Exception as e:
                                failed_files.append(f"ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    
                    progress_bar.progress((item_idx + 1) / len(all_items))
                
                st.session_state.notion_files = downloaded_files
                st.success(f"âœ… {len(downloaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
                
                if failed_files:
                    st.warning(f"âš ï¸ {len(failed_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                    with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°"):
                        for error in failed_files:
                            st.write(f"- {error}")
                
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def file_upload_processing():
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†"""
    st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»çµ±åˆå‡¦ç†")
    
    # Notionã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
    if 'notion_files' in st.session_state and st.session_state.notion_files:
        st.info(f"ğŸ’¾ Notionã‹ã‚‰{len(st.session_state.notion_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“¥ Notionãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"):
                process_files(st.session_state.notion_files, "Notion")
        with col2:
            if st.button("ğŸ—‘ï¸ Notionãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢"):
                del st.session_state.notion_files
                st.rerun()
    
    uploaded_files = st.file_uploader(
        "CSVã¾ãŸã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.write(f"ğŸ“‚ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(uploaded_files)}")
        
        if st.button("ğŸ”„ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"):
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¤‰æ›
            file_data = []
            for uploaded_file in uploaded_files:
                content = uploaded_file.read()
                uploaded_file.seek(0)  # ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                file_data.append((uploaded_file.name, content))
            
            process_files(file_data, "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

def process_files(file_data, source_type):
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
    processed_dfs = []
    error_files = []
    total_stats = {"email_extracted": 0, "tel_extracted": 0, "files_processed": 0}
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, (filename, content) in enumerate(file_data):
        status_text.text(f"å‡¦ç†ä¸­: {filename}")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            if filename.lower().endswith('.csv'):
                encoding = detect(content)["encoding"] or "utf-8"
                content_str = content.decode(encoding)
                df = pd.read_csv(io.StringIO(content_str), dtype=str, on_bad_lines="skip")
            else:
                df = pd.read_excel(io.BytesIO(content), dtype=str, engine='openpyxl')
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            processed_df, stats, error = process_dataframe(df, filename)
            
            if processed_df is not None:
                processed_dfs.append(processed_df)
                total_stats["email_extracted"] += stats["email_extracted"]
                total_stats["tel_extracted"] += stats["tel_extracted"]
                total_stats["files_processed"] += 1
                
                st.session_state.processed_files.append({
                    'filename': filename,
                    'rows': len(processed_df),
                    'email_extracted': stats["email_extracted"],
                    'tel_extracted': stats["tel_extracted"],
                    'status': 'âœ… æˆåŠŸ'
                })
            else:
                error_files.append((filename, error))
                st.session_state.processed_files.append({
                    'filename': filename,
                    'rows': 0,
                    'email_extracted': 0,
                    'tel_extracted': 0,
                    'status': f'âŒ ã‚¨ãƒ©ãƒ¼: {error}'
                })
                
        except Exception as e:
            error_files.append((filename, str(e)))
            st.session_state.processed_files.append({
                'filename': filename,
                'rows': 0,
                'email_extracted': 0,
                'tel_extracted': 0,
                'status': f'âŒ ã‚¨ãƒ©ãƒ¼: {e}'
            })
        
        progress_bar.progress((idx + 1) / len(file_data))
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    if processed_dfs:
        merged_df = pd.concat(processed_dfs, ignore_index=True)
        
        # é‡è¤‡å‰Šé™¤ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
        before_count = len(merged_df)
        if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in merged_df.columns:
            email_duplicates = merged_df[merged_df['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != ''].duplicated(
                subset=['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'], keep='first'
            )
            merged_df = merged_df[~email_duplicates]
        
        # ç©ºè¡Œå‰Šé™¤
        key_cols_check = ["ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "Tel", "ä¼šç¤¾å"]
        empty_mask = merged_df[key_cols_check].replace("", pd.NA).isna().all(axis=1)
        merged_df = merged_df[~empty_mask]
        
        after_count = len(merged_df)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
        if not st.session_state.merged_data.empty:
            st.session_state.merged_data = pd.concat([st.session_state.merged_data, merged_df], ignore_index=True)
            # å…¨ä½“ã§ã‚‚é‡è¤‡å‰Šé™¤
            if 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in st.session_state.merged_data.columns:
                email_duplicates = st.session_state.merged_data[st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != ''].duplicated(
                    subset=['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'], keep='last'
                )
                st.session_state.merged_data = st.session_state.merged_data[~email_duplicates]
        else:
            st.session_state.merged_data = merged_df
        
        # çµ±è¨ˆæƒ…å ±ä¿å­˜
        st.session_state.processing_stats = total_stats
        
        st.success(f"""
        âœ… **{source_type}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†**
        - å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_stats['files_processed']}å€‹
        - å‡¦ç†å‰ãƒ‡ãƒ¼ã‚¿æ•°: {before_count}ä»¶
        - é‡è¤‡ãƒ»ç©ºè¡Œå‰Šé™¤å¾Œ: {after_count}ä»¶
        - ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºæ•°: {total_stats['email_extracted']}ä»¶
        - é›»è©±ç•ªå·æŠ½å‡ºæ•°: {total_stats['tel_extracted']}ä»¶
        - æœ€çµ‚çµ±åˆãƒ‡ãƒ¼ã‚¿æ•°: {len(st.session_state.merged_data)}ä»¶
        """)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º
    if error_files:
        st.error("âŒ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:")
        for filename, error in error_files:
            st.write(f"- **{filename}**: {error}")
    
    status_text.empty()
    progress_bar.empty()

def display_processed_files():
    """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º"""
    if st.session_state.processed_files:
        st.subheader("ğŸ“‹ å‡¦ç†çµæœè©³ç´°")
        df_status = pd.DataFrame(st.session_state.processed_files)
        st.dataframe(df_status, use_container_width=True)
        
        # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
        if st.session_state.processing_stats:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å‡¦ç†æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«", st.session_state.processing_stats.get('files_processed', 0))
            with col2:
                st.metric("ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºæ•°", st.session_state.processing_stats.get('email_extracted', 0))
            with col3:
                st.metric("é›»è©±ç•ªå·æŠ½å‡ºæ•°", st.session_state.processing_stats.get('tel_extracted', 0))

def data_search_and_download():
    """ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½"""
    if st.session_state.merged_data.empty:
        st.info("âš ï¸ ã¾ãšãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦çµ±åˆã—ã¦ãã ã•ã„")
        return
        
    st.subheader("ğŸ” é«˜åº¦ãªæ¤œç´¢ãƒ»åˆ†æãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(st.session_state.merged_data))
    with col2:
        unique_companies = st.session_state.merged_data['ä¼šç¤¾å'].nunique()
        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°", unique_companies)
    with col3:
        unique_exhibitions = st.session_state.merged_data['å±•ç¤ºä¼šå'].nunique()
        st.metric("å±•ç¤ºä¼šæ•°", unique_exhibitions)
    with col4:
        email_with_data = len(st.session_state.merged_data[
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & 
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
        ])
        st.metric("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚ã‚Š", email_with_data)
    
    # è©³ç´°çµ±è¨ˆ
    with st.expander("ğŸ“Š è©³ç´°çµ±è¨ˆæƒ…å ±"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**å±•ç¤ºä¼šåˆ¥ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆä¸Šä½10ä½ï¼‰**")
            exhibition_counts = st.session_state.merged_data['å±•ç¤ºä¼šå'].value_counts().head(10)
            for idx, (exhibition, count) in enumerate(exhibition_counts.items(), 1):
                st.write(f"{idx}. {exhibition}: {count}ä»¶")
        
        with col2:
            st.write("**æ¥­ç•Œåˆ¥ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆä¸Šä½10ä½ï¼‰**")
            industry_counts = st.session_state.merged_data['æ¥­ç•Œ'].value_counts().head(10)
            for idx, (industry, count) in enumerate(industry_counts.items(), 1):
                st.write(f"{idx}. {industry}: {count}ä»¶")
    
    # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.markdown("### ğŸ¯ è©³ç´°æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    col1, col2 = st.columns(2)
    
    with col1:
        # å±•ç¤ºä¼šåãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        exhibitions = ['å…¨ã¦'] + sorted(st.session_state.merged_data['å±•ç¤ºä¼šå'].dropna().unique().tolist())
        selected_exhibitions = st.multiselect("å±•ç¤ºä¼šåï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", exhibitions, default=['å…¨ã¦'])
        
        # æ¥­ç•Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        industries = ['å…¨ã¦'] + sorted(st.session_state.merged_data['æ¥­ç•Œ'].dropna().unique().tolist())
        selected_industries = st.multiselect("æ¥­ç•Œï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", industries, default=['å…¨ã¦'])
        
        # ä¼šç¤¾åæ¤œç´¢
        company_search = st.text_input("ä¼šç¤¾åæ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")
    
    with col2:
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æœ‰ç„¡
        email_filter = st.selectbox("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", ['å…¨ã¦', 'ã‚ã‚Š', 'ãªã—'])
        
        # é›»è©±ç•ªå·æœ‰ç„¡
        tel_filter = st.selectbox("é›»è©±ç•ªå·", ['å…¨ã¦', 'ã‚ã‚Š', 'ãªã—'])
        
        # æ‹…å½“è€…æ¤œç´¢
        contact_search = st.text_input("æ‹…å½“è€…åæ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_data = st.session_state.merged_data.copy()
    
    # è¤‡æ•°é¸æŠå¯¾å¿œ
    if 'å…¨ã¦' not in selected_exhibitions:
        filtered_data = filtered_data[filtered_data['å±•ç¤ºä¼šå'].isin(selected_exhibitions)]
    
    if 'å…¨ã¦' not in selected_industries:
        filtered_data = filtered_data[filtered_data['æ¥­ç•Œ'].isin(selected_industries)]
    
    if company_search:
        filtered_data = filtered_data[
            filtered_data['ä¼šç¤¾å'].str.contains(company_search, na=False, case=False)
        ]
    
    if contact_search:
        filtered_data = filtered_data[
            filtered_data['æ‹…å½“è€…'].str.contains(contact_search, na=False, case=False)
        ]
    
    if email_filter == 'ã‚ã‚Š':
        filtered_data = filtered_data[
            filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna() & 
            (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
        ]
    elif email_filter == 'ãªã—':
        filtered_data = filtered_data[
            filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].isna() | 
            (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] == '')
        ]
    
    if tel_filter == 'ã‚ã‚Š':
        filtered_data = filtered_data[
            filtered_data['Tel'].notna() & 
            (filtered_data['Tel'] != '')
        ]
    elif tel_filter == 'ãªã—':
        filtered_data = filtered_data[
            filtered_data['Tel'].isna() | 
            (filtered_data['Tel'] == '')
        ]
    
    # æ¤œç´¢çµæœè¡¨ç¤º
    st.markdown(f"### ğŸ¯ æ¤œç´¢çµæœ: **{len(filtered_data)}ä»¶**")
    
    if not filtered_data.empty:
        # çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¨­å®š
        preview_limit = st.slider("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºä»¶æ•°", 10, 1000, 100)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        st.dataframe(filtered_data.head(preview_limit), use_container_width=True)
        
        if len(filtered_data) > preview_limit:
            st.info(f"ğŸ’¡ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯æœ€åˆã®{preview_limit}ä»¶ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚å…¨{len(filtered_data)}ä»¶")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.markdown("### ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            csv = filtered_data.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“„ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"exhibition_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
        with col2:
            # Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                filtered_data.to_excel(writer, index=False, sheet_name='ExhibitionData')
            excel_data = output.getvalue()
            
            st.download_button(
                label="ğŸ“Š Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=excel_data,
                file_name=f"exhibition_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        
        with col3:
            # ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã®ã¿
            email_only = filtered_data[
                (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & 
                (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
            ][['ä¼šç¤¾å', 'æ‹…å½“è€…', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'å±•ç¤ºä¼šå', 'æ¥­ç•Œ']]
            
            email_csv = email_only.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“§ ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ",
                data=email_csv,
                file_name=f"email_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
        with col4:
            # ãƒ†ãƒ¬ã‚¢ãƒãƒªã‚¹ãƒˆ
            tel_only = filtered_data[
                (filtered_data['Tel'].notna()) & 
                (filtered_data['Tel'] != '')
            ][['ä¼šç¤¾å', 'æ‹…å½“è€…', 'Tel', 'å±•ç¤ºä¼šå', 'æ¥­ç•Œ']]
            
            tel_csv = tel_only.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“ ãƒ†ãƒ¬ã‚¢ãƒãƒªã‚¹ãƒˆ",
                data=tel_csv,
                file_name=f"tel_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
        # è©³ç´°çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
        with st.expander("ğŸ“ˆ æ¤œç´¢çµæœã®è©³ç´°åˆ†æ"):
            col1, col2 = st.columns(2)
            
            with col1:
                stats_data = {
                    'é …ç›®': [
                        'ç·ãƒ‡ãƒ¼ã‚¿æ•°', 'ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°', 'å±•ç¤ºä¼šæ•°', 'æ¥­ç•Œæ•°', 
                        'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚ã‚Š', 'é›»è©±ç•ªå·ã‚ã‚Š', 'ä¸¡æ–¹ã‚ã‚Š'
                    ],
                    'ä»¶æ•°': [
                        len(filtered_data),
                        filtered_data['ä¼šç¤¾å'].nunique(),
                        filtered_data['å±•ç¤ºä¼šå'].nunique(),
                        filtered_data['æ¥­ç•Œ'].nunique(),
                        len(filtered_data[(filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')]),
                        len(filtered_data[(filtered_data['Tel'].notna()) & (filtered_data['Tel'] != '')]),
                        len(filtered_data[
                            (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & (filtered_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '') &
                            (filtered_data['Tel'].notna()) & (filtered_data['Tel'] != '')
                        ])
                    ]
                }
                stats_df = pd.DataFrame(stats_data)
                st.dataframe(stats_df, use_container_width=True)
            
            with col2:
                # æ›´æ–°æ—¥æ™‚åˆ¥é›†è¨ˆ
                if 'æ›´æ–°æ—¥æ™‚' in filtered_data.columns:
                    st.write("**æ›´æ–°æ—¥æ™‚åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°**")
                    update_counts = filtered_data['æ›´æ–°æ—¥æ™‚'].value_counts().head(10)
                    for date, count in update_counts.items():
                        st.write(f"- {date}: {count}ä»¶")
    else:
        st.warning("ğŸ” æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

def main():
    st.title("ğŸ“Š å±•ç¤ºä¼šãƒªã‚¹ãƒˆè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé«˜æ©Ÿèƒ½ç‰ˆï¼‰")
    st.markdown("""
    **ğŸš€ æ©Ÿèƒ½ä¸€è¦§:**
    - Notionã‹ã‚‰ã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGoogleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå¯¾å¿œï¼‰
    - é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆé€£çµ¡å…ˆã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ãƒ»é›»è©±ç•ªå·è‡ªå‹•æŠ½å‡ºï¼‰
    - åˆ—åã®è‡ªå‹•æ­£è¦åŒ–ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    - é‡è¤‡å‰Šé™¤ãƒ»ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    - é«˜åº¦ãªæ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - ãƒ¡ãƒ¼ãƒ«ãƒªã‚¹ãƒˆãƒ»ãƒ†ãƒ¬ã‚¢ãƒãƒªã‚¹ãƒˆç”Ÿæˆ
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    st.sidebar.title("ğŸ”§ æ©Ÿèƒ½é¸æŠ")
    mode = st.sidebar.radio(
        "å‡¦ç†ã‚’é¸æŠã—ã¦ãã ã•ã„:",
        ["ğŸ”— Notioné€£æº", "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†", "ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»åˆ†æ"]
    )
    
    if mode == "ğŸ”— Notioné€£æº":
        notion_download()
    elif mode == "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†":
        file_upload_processing()
        display_processed_files()
    elif mode == "ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãƒ»åˆ†æ":
        data_search_and_download()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®çµ±è¨ˆæƒ…å ±
    if not st.session_state.merged_data.empty:
        st.sidebar.markdown("---")
        st.sidebar.markdown("ğŸ“Š **ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³**")
        st.sidebar.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(st.session_state.merged_data))
        st.sidebar.metric("ä¼æ¥­æ•°", st.session_state.merged_data['ä¼šç¤¾å'].nunique())
        email_count = len(st.session_state.merged_data[
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].notna()) & 
            (st.session_state.merged_data['ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'] != '')
        ])
        st.sidebar.metric("ãƒ¡ãƒ¼ãƒ«ã‚ã‚Š", email_count)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ
    if st.sidebar.button("ğŸ”„ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state.merged_data = pd.DataFrame()
        st.session_state.processed_files = []
        st.session_state.processing_stats = {}
        if 'notion_files' in st.session_state:
            del st.session_state.notion_files
        st.sidebar.success("âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
        st.rerun()

if __name__ == "__main__":
    main()
